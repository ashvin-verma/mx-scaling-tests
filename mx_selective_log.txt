2025-10-02 23:39:44,610 - INFO - Will replace only targeted modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'c_attn', 'c_proj', 'c_fc']
2025-10-02 23:39:45,711 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3'}
2025-10-02 23:39:45,711 - INFO - Loading evaluation data...
2025-10-02 23:39:46,388 - WARNING - Could not load The Pile: Dataset scripts are no longer supported, but found pile.py. Falling back to WikiText.
2025-10-02 23:39:48,642 - INFO - ================================================================================
2025-10-02 23:39:48,642 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-02 23:39:48,642 - INFO - ================================================================================
2025-10-02 23:39:48,642 - INFO - 
================================================================================
2025-10-02 23:39:48,642 - INFO - Evaluating: tinystories-33M
2025-10-02 23:39:48,643 - INFO - ================================================================================
2025-10-02 23:39:48,643 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-02 23:39:50,608 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:40:07,235 - INFO - tinystories-33M                 Baseline         PPL 2.84  Xent 1.0438  Entr 2.9906
2025-10-02 23:40:17,751 - INFO - 
--- MX (Selective GEMM) ---
2025-10-02 23:40:17,791 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:40:19,108 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-02 23:40:19,108 - INFO - Replaced 0 Linear layers with MxLinear
2025-10-02 23:40:29,228 - INFO - tinystories-33M-MX              MX-Selective     PPL 2.84  Xent 1.0438  Entr 2.9906
2025-10-02 23:40:39,722 - INFO - 
================================================================================
2025-10-02 23:40:39,722 - INFO - Evaluating: qwen1.5-0.5B
2025-10-02 23:40:39,722 - INFO - ================================================================================
2025-10-02 23:40:39,723 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-02 23:40:39,724 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:41:47,245 - INFO - qwen1.5-0.5B                    Baseline         PPL 9.08  Xent 2.2062  Entr 5.4072
2025-10-02 23:41:59,082 - INFO - 
--- MX (Selective GEMM) ---
2025-10-02 23:41:59,086 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:42:00,542 - INFO - Applying MX quantization to Qwen/Qwen1.5-0.5B
2025-10-02 23:42:00,543 - INFO - Replaced 0 Linear layers with MxLinear
2025-10-02 23:42:55,432 - INFO - qwen1.5-0.5B-MX                 MX-Selective     PPL 9.08  Xent 2.2062  Entr 5.4072
2025-10-02 23:43:07,138 - INFO - 
================================================================================
2025-10-02 23:43:07,226 - INFO - Evaluating: phi-1_5
2025-10-02 23:43:07,226 - INFO - ================================================================================
2025-10-02 23:43:07,227 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-02 23:43:07,229 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:45:37,988 - INFO - phi-1_5                         Baseline         PPL 2.08  Xent 0.7337  Entr 2.5252
2025-10-02 23:45:52,387 - INFO - 
--- MX (Selective GEMM) ---
2025-10-02 23:45:52,388 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:45:53,997 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-02 23:45:53,998 - INFO - Replaced 0 Linear layers with MxLinear
2025-10-02 23:47:58,880 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.08  Xent 0.7337  Entr 2.5252
2025-10-02 23:48:13,449 - INFO - 
================================================================================
2025-10-02 23:48:13,450 - INFO - Evaluating: qwen2.5-7b
2025-10-02 23:48:13,450 - INFO - ================================================================================
2025-10-02 23:48:13,450 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-02 23:48:13,452 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:52:29,706 - INFO - qwen2.5-7b                      Baseline         PPL 3.35  Xent 1.2083  Entr 4.0024
2025-10-02 23:53:29,304 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-02 23:53:30,465 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3'}
2025-10-02 23:53:30,466 - INFO - Loading evaluation data...
2025-10-02 23:53:30,466 - INFO - Loading 50 samples from The Pile (validation split)...
2025-10-02 23:53:33,021 - WARNING - Could not load The Pile: Bad split: validation. Available splits: ['train']. Falling back to WikiText.
2025-10-02 23:53:35,050 - INFO - ================================================================================
2025-10-02 23:53:35,051 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-02 23:53:35,051 - INFO - ================================================================================
2025-10-02 23:53:35,051 - INFO - 
================================================================================
2025-10-02 23:53:35,051 - INFO - Evaluating: tinystories-33M
2025-10-02 23:53:35,051 - INFO - ================================================================================
2025-10-02 23:53:35,051 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-02 23:53:37,000 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:53:39,807 - INFO - tinystories-33M                 Baseline         PPL 2.39  Xent 0.8713  Entr 2.4317
2025-10-02 23:53:50,431 - INFO - 
--- MX (Selective GEMM) ---
2025-10-02 23:53:50,479 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:53:51,970 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-02 23:53:51,971 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-02 23:54:35,331 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-02 23:54:36,490 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-02 23:54:36,491 - INFO - Loading evaluation data...
2025-10-02 23:54:36,491 - INFO - Loading 50 samples from The Pile (train split)...
2025-10-02 23:54:38,938 - WARNING - Could not load The Pile: Compression type zstd not supported. Falling back to WikiText.
2025-10-02 23:54:40,926 - INFO - ================================================================================
2025-10-02 23:54:40,926 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-02 23:54:40,926 - INFO - ================================================================================
2025-10-02 23:54:40,926 - INFO - 
================================================================================
2025-10-02 23:54:40,926 - INFO - Evaluating: tinystories-33M
2025-10-02 23:54:40,926 - INFO - ================================================================================
2025-10-02 23:54:40,927 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-02 23:54:42,805 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-02 23:54:45,589 - INFO - tinystories-33M                 Baseline         PPL 2.39  Xent 0.8713  Entr 2.4317
2025-10-03 00:06:34,210 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 00:06:35,432 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 00:06:35,433 - INFO - Loading evaluation data...
2025-10-03 00:06:35,433 - INFO - Loading 100 samples from The Pile (train split)...
2025-10-03 00:06:37,918 - WARNING - Could not load The Pile: Compression type zstd not supported. Falling back to WikiText.
2025-10-03 00:06:40,525 - INFO - ================================================================================
2025-10-03 00:06:40,525 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 00:06:40,525 - INFO - ================================================================================
2025-10-03 00:06:40,525 - INFO - 
================================================================================
2025-10-03 00:06:40,525 - INFO - Evaluating: tinystories-33M
2025-10-03 00:06:40,525 - INFO - ================================================================================
2025-10-03 00:06:40,526 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:06:42,380 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:06:45,553 - INFO - tinystories-33M                 Baseline         PPL 2.49  Xent 0.9103  Entr 2.5539
2025-10-03 00:06:56,026 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:06:56,064 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:06:57,594 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 00:06:57,594 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 00:06:59,731 - INFO - tinystories-33M-MX              MX-Selective     PPL 2.54  Xent 0.9322  Entr 2.6054
2025-10-03 00:07:10,168 - INFO - 
================================================================================
2025-10-03 00:07:10,168 - INFO - BENCHMARK COMPLETE
2025-10-03 00:07:10,169 - INFO - ================================================================================
2025-10-03 00:28:32,039 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 00:28:33,308 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 00:28:33,308 - INFO - Loading evaluation data...
2025-10-03 00:28:33,308 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 00:28:35,766 - WARNING - Could not load The Pile: Compression type zstd not supported. Falling back to WikiText.
2025-10-03 00:28:37,836 - INFO - ================================================================================
2025-10-03 00:28:37,837 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 00:28:37,837 - INFO - ================================================================================
2025-10-03 00:28:37,837 - INFO - 
================================================================================
2025-10-03 00:28:37,837 - INFO - Evaluating: tinystories-33M
2025-10-03 00:28:37,837 - INFO - ================================================================================
2025-10-03 00:28:37,837 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:28:39,746 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:28:52,075 - INFO - tinystories-33M                 Baseline         PPL 2.84  Xent 1.0438  Entr 2.9906
2025-10-03 00:29:02,619 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:29:02,667 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:29:04,072 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 00:29:04,073 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 00:29:15,169 - INFO - tinystories-33M-MX              MX-Selective     PPL 2.92  Xent 1.0718  Entr 3.0512
2025-10-03 00:29:25,659 - INFO - 
================================================================================
2025-10-03 00:29:25,659 - INFO - Evaluating: qwen1.5-0.5B
2025-10-03 00:29:25,659 - INFO - ================================================================================
2025-10-03 00:29:25,659 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:29:25,662 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:30:21,808 - INFO - qwen1.5-0.5B                    Baseline         PPL 9.08  Xent 2.2062  Entr 5.4072
2025-10-03 00:30:33,623 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:30:33,632 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:30:35,120 - INFO - Applying MX quantization to Qwen/Qwen1.5-0.5B
2025-10-03 00:30:35,122 - INFO - Replaced 72 Linear layers with MxLinear
2025-10-03 00:31:31,204 - INFO - qwen1.5-0.5B-MX                 MX-Selective     PPL 26.41  Xent 3.2738  Entr 6.7087
2025-10-03 00:31:43,015 - INFO - 
================================================================================
2025-10-03 00:31:43,016 - INFO - Evaluating: phi-1_5
2025-10-03 00:31:43,016 - INFO - ================================================================================
2025-10-03 00:31:43,016 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:31:43,017 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:33:48,170 - INFO - phi-1_5                         Baseline         PPL 2.08  Xent 0.7337  Entr 2.5252
2025-10-03 00:34:02,943 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:34:02,945 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:34:04,591 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-03 00:34:04,593 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-03 00:36:13,350 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.21  Xent 0.7941  Entr 2.7995
2025-10-03 00:36:28,114 - INFO - 
================================================================================
2025-10-03 00:36:28,114 - INFO - BENCHMARK COMPLETE
2025-10-03 00:36:28,115 - INFO - ================================================================================
2025-10-03 00:51:36,838 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 00:51:38,083 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 00:51:38,084 - INFO - Loading evaluation data...
2025-10-03 00:51:38,084 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 00:51:41,064 - INFO - Loaded 1000 samples from The Pile
2025-10-03 00:51:41,065 - INFO - ================================================================================
2025-10-03 00:51:41,065 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 00:51:41,065 - INFO - ================================================================================
2025-10-03 00:51:41,065 - INFO - 
================================================================================
2025-10-03 00:51:41,065 - INFO - Evaluating: tinystories-33M
2025-10-03 00:51:41,065 - INFO - ================================================================================
2025-10-03 00:51:41,065 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:51:42,873 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:51:55,515 - INFO - tinystories-33M                 Baseline         PPL 5.09  Xent 1.6270  Entr 4.6149
2025-10-03 00:52:06,062 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:52:06,122 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:52:07,411 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 00:52:07,412 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 00:52:19,384 - INFO - tinystories-33M-MX              MX-Selective     PPL 5.20  Xent 1.6493  Entr 4.6689
2025-10-03 00:52:29,865 - INFO - 
================================================================================
2025-10-03 00:52:29,866 - INFO - Evaluating: qwen1.5-0.5B
2025-10-03 00:52:29,866 - INFO - ================================================================================
2025-10-03 00:52:29,866 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:52:29,869 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:53:27,002 - INFO - qwen1.5-0.5B                    Baseline         PPL 3.39  Xent 1.2198  Entr 3.2130
2025-10-03 00:53:38,839 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:53:38,840 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:53:40,290 - INFO - Applying MX quantization to Qwen/Qwen1.5-0.5B
2025-10-03 00:53:40,292 - INFO - Replaced 72 Linear layers with MxLinear
2025-10-03 00:54:38,047 - INFO - qwen1.5-0.5B-MX                 MX-Selective     PPL 3.30  Xent 1.1937  Entr 3.1290
2025-10-03 00:54:49,892 - INFO - 
================================================================================
2025-10-03 00:54:49,893 - INFO - Evaluating: phi-1_5
2025-10-03 00:54:49,893 - INFO - ================================================================================
2025-10-03 00:54:49,893 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 00:54:49,899 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:56:59,431 - INFO - phi-1_5                         Baseline         PPL 2.36  Xent 0.8589  Entr 2.4506
2025-10-03 00:57:14,358 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 00:57:14,398 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 00:57:16,118 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-03 00:57:16,120 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-03 00:59:27,407 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.41  Xent 0.8810  Entr 2.5410
2025-10-03 00:59:42,218 - INFO - 
================================================================================
2025-10-03 00:59:42,218 - INFO - BENCHMARK COMPLETE
2025-10-03 00:59:42,218 - INFO - ================================================================================
2025-10-03 01:06:42,857 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 01:06:44,019 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 01:06:44,019 - INFO - Loading evaluation data...
2025-10-03 01:06:44,019 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 01:06:47,172 - INFO - Loaded 1000 samples from The Pile
2025-10-03 01:06:47,172 - INFO - ================================================================================
2025-10-03 01:06:47,172 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 01:06:47,172 - INFO - ================================================================================
2025-10-03 01:06:47,172 - INFO - 
================================================================================
2025-10-03 01:06:47,172 - INFO - Evaluating: llama-3.1-8b
2025-10-03 01:06:47,172 - INFO - ================================================================================
2025-10-03 01:06:47,173 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:06:48,993 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:06:51,173 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 01:06:51,677 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 01:06:51,678 - INFO - Loading evaluation data...
2025-10-03 01:06:51,678 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 01:06:55,149 - INFO - Loaded 1000 samples from The Pile
2025-10-03 01:06:55,149 - INFO - ================================================================================
2025-10-03 01:06:55,150 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 01:06:55,150 - INFO - ================================================================================
2025-10-03 01:06:55,150 - INFO - 
================================================================================
2025-10-03 01:06:55,150 - INFO - Evaluating: llama-3.1-8b
2025-10-03 01:06:55,150 - INFO - ================================================================================
2025-10-03 01:06:55,150 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:06:56,981 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:12:52,688 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 01:12:53,902 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 01:12:53,902 - INFO - Loading evaluation data...
2025-10-03 01:12:53,902 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 01:12:57,440 - INFO - Loaded 1000 samples from The Pile
2025-10-03 01:12:57,441 - INFO - ================================================================================
2025-10-03 01:12:57,441 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 01:12:57,441 - INFO - ================================================================================
2025-10-03 01:12:57,441 - INFO - 
================================================================================
2025-10-03 01:12:57,441 - INFO - Evaluating: llama-3.1-8b
2025-10-03 01:12:57,441 - INFO - ================================================================================
2025-10-03 01:12:57,441 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:12:59,360 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:14:16,913 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 01:14:18,096 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 01:14:18,097 - INFO - Loading evaluation data...
2025-10-03 01:14:18,097 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 01:14:21,165 - INFO - Loaded 1000 samples from The Pile
2025-10-03 01:14:21,165 - INFO - ================================================================================
2025-10-03 01:14:21,165 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 01:14:21,165 - INFO - ================================================================================
2025-10-03 01:14:21,165 - INFO - 
================================================================================
2025-10-03 01:14:21,165 - INFO - Evaluating: qwen2.5-7b
2025-10-03 01:14:21,166 - INFO - ================================================================================
2025-10-03 01:14:21,166 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:17:30,716 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 01:17:31,915 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 01:17:31,915 - INFO - Loading evaluation data...
2025-10-03 01:17:31,915 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 01:17:35,106 - INFO - Loaded 1000 samples from The Pile
2025-10-03 01:17:35,107 - INFO - ================================================================================
2025-10-03 01:17:35,107 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 01:17:35,107 - INFO - ================================================================================
2025-10-03 01:17:35,107 - INFO - 
================================================================================
2025-10-03 01:17:35,107 - INFO - Evaluating: qwen2.5-7b
2025-10-03 01:17:35,107 - INFO - ================================================================================
2025-10-03 01:17:35,107 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:17:36,988 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:19:39,144 - INFO - qwen2.5-7b                      Baseline         PPL 2.87  Xent 1.0552  Entr 2.8286
2025-10-03 01:20:00,909 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 01:20:00,954 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:20:02,123 - INFO - Applying MX quantization to Qwen/Qwen2.5-7B
2025-10-03 01:20:02,125 - INFO - Replaced 84 Linear layers with MxLinear
2025-10-03 01:22:48,107 - INFO - qwen2.5-7b-MX                   MX-Selective     PPL 2.87  Xent 1.0546  Entr 2.8399
2025-10-03 01:23:09,981 - INFO - 
================================================================================
2025-10-03 01:23:09,981 - INFO - Evaluating: qwen2.5-14b
2025-10-03 01:23:09,982 - INFO - ================================================================================
2025-10-03 01:23:09,982 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:23:09,984 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:48:53,298 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 01:48:54,491 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 01:48:54,492 - INFO - Loading evaluation data...
2025-10-03 01:48:54,492 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 01:48:58,710 - INFO - Loaded 1000 samples from The Pile
2025-10-03 01:48:58,710 - INFO - ================================================================================
2025-10-03 01:48:58,710 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 01:48:58,710 - INFO - ================================================================================
2025-10-03 01:48:58,710 - INFO - 
================================================================================
2025-10-03 01:48:58,710 - INFO - Evaluating: llama-3.1-8b
2025-10-03 01:48:58,710 - INFO - ================================================================================
2025-10-03 01:48:58,710 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:49:00,579 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:53:10,050 - INFO - llama-3.1-8b                    Baseline         PPL 1.73  Xent 0.5484  Entr 1.5210
2025-10-03 01:53:32,486 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 01:53:32,536 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 01:53:34,161 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 01:53:34,164 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-03 01:56:07,034 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.75  Xent 0.5609  Entr 1.5969
2025-10-03 01:56:29,470 - INFO - 
================================================================================
2025-10-03 01:56:29,471 - INFO - Evaluating: qwen2.5-14b
2025-10-03 01:56:29,471 - INFO - ================================================================================
2025-10-03 01:56:29,471 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 01:56:29,473 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 08:57:29,198 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 08:57:29,889 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 08:57:29,889 - INFO - Loading evaluation data...
2025-10-03 08:57:29,889 - INFO - Loading 10 samples from The Pile (train split)...
2025-10-03 08:57:48,550 - INFO - Loaded 10 samples from The Pile
2025-10-03 08:57:48,551 - INFO - ================================================================================
2025-10-03 08:57:48,551 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 08:57:48,551 - INFO - ================================================================================
2025-10-03 08:57:48,551 - INFO - 
================================================================================
2025-10-03 08:57:48,551 - INFO - Evaluating: tinystories-33M
2025-10-03 08:57:48,551 - INFO - ================================================================================
2025-10-03 08:57:48,551 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 08:57:50,580 - INFO - Baseline         Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 08:57:53,228 - INFO - tinystories-33M                 Baseline         PPL 4.68  Xent 1.5431  Entr 4.3353
2025-10-03 08:58:03,735 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 08:58:03,775 - INFO - MX-Selective     Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 08:58:05,259 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 08:58:05,260 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 08:58:06,080 - INFO - tinystories-33M-MX              MX-Selective     PPL 4.70  Xent 1.5474  Entr 4.3424
2025-10-03 08:58:16,513 - INFO - 
================================================================================
2025-10-03 08:58:16,513 - INFO - BENCHMARK COMPLETE
2025-10-03 08:58:16,513 - INFO - ================================================================================
2025-10-03 09:25:16,590 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 09:25:17,259 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 09:25:17,260 - INFO - Loading evaluation data...
2025-10-03 09:25:17,260 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 09:25:20,793 - INFO - Loaded 1000 samples from The Pile
2025-10-03 09:25:20,793 - INFO - ================================================================================
2025-10-03 09:25:20,793 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 09:25:20,793 - INFO - ================================================================================
2025-10-03 09:25:20,793 - INFO - 
================================================================================
2025-10-03 09:25:20,793 - INFO - Evaluating: llama-3.1-8b
2025-10-03 09:25:20,793 - INFO - ================================================================================
2025-10-03 09:25:20,793 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 09:25:22,852 - INFO - Baseline         Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:27:30,579 - INFO - llama-3.1-8b                    Baseline         PPL 1.73  Xent 0.5484  Entr 1.5210
2025-10-03 09:27:53,933 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 09:27:53,981 - INFO - MX-Selective     Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:27:55,558 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 09:27:55,560 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-03 09:30:27,817 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.75  Xent 0.5609  Entr 1.5969
2025-10-03 09:30:51,167 - INFO - 
================================================================================
2025-10-03 09:30:51,167 - INFO - Evaluating: qwen2.5-14b
2025-10-03 09:30:51,167 - INFO - ================================================================================
2025-10-03 09:30:51,167 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 09:30:51,169 - INFO - Baseline         Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:51:19,988 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 09:51:20,660 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 09:51:20,660 - INFO - Loading evaluation data...
2025-10-03 09:51:20,660 - INFO - Loading 10 samples from The Pile (train split)...
2025-10-03 09:51:23,951 - INFO - Loaded 10 samples from The Pile
2025-10-03 09:51:23,951 - INFO - ================================================================================
2025-10-03 09:51:23,951 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 09:51:23,952 - INFO - ================================================================================
2025-10-03 09:51:23,952 - INFO - 
================================================================================
2025-10-03 09:51:23,952 - INFO - Evaluating: tinystories-33M
2025-10-03 09:51:23,952 - INFO - ================================================================================
2025-10-03 09:51:23,952 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 09:51:26,012 - INFO - Baseline         Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:51:28,384 - INFO - tinystories-33M                 Baseline         PPL 4.68  Xent 1.5431  Entr 4.3353
2025-10-03 09:51:38,885 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 09:51:38,918 - INFO - MX-Selective     Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:51:40,311 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 09:51:40,312 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 09:51:41,110 - INFO - tinystories-33M-MX              MX-Selective     PPL 4.70  Xent 1.5474  Entr 4.3424
2025-10-03 09:51:51,550 - INFO - 
================================================================================
2025-10-03 09:51:51,550 - INFO - BENCHMARK COMPLETE
2025-10-03 09:51:51,551 - INFO - ================================================================================
2025-10-03 09:52:28,918 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 09:52:29,587 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 09:52:29,588 - INFO - Loading evaluation data...
2025-10-03 09:52:29,588 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 09:52:32,916 - INFO - Loaded 1000 samples from The Pile
2025-10-03 09:52:32,917 - INFO - ================================================================================
2025-10-03 09:52:32,917 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 09:52:32,917 - INFO - ================================================================================
2025-10-03 09:52:32,917 - INFO - 
================================================================================
2025-10-03 09:52:32,917 - INFO - Evaluating: llama-3.1-8b
2025-10-03 09:52:32,917 - INFO - ================================================================================
2025-10-03 09:52:32,917 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 09:52:34,926 - INFO - Baseline         Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:54:42,625 - INFO - llama-3.1-8b                    Baseline         PPL 1.73  Xent 0.5484  Entr 1.5210
2025-10-03 09:55:06,322 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 09:55:06,374 - INFO - MX-Selective     Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 09:55:07,963 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 09:55:07,965 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-03 09:57:40,591 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.75  Xent 0.5609  Entr 1.5969
2025-10-03 09:58:03,827 - INFO - 
================================================================================
2025-10-03 09:58:03,827 - INFO - Evaluating: qwen2.5-14b
2025-10-03 09:58:03,827 - INFO - ================================================================================
2025-10-03 09:58:03,827 - INFO - Using batch size override 2 for qwen2.5-14b
2025-10-03 09:58:03,828 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 09:58:03,830 - INFO - Baseline         Using GPUs: [2, 4, 5, 6] with 1 worker(s)/device
2025-10-03 10:09:46,337 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 10:09:47,453 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 10:09:47,453 - INFO - Loading evaluation data...
2025-10-03 10:09:47,453 - INFO - Loading 10 samples from The Pile (train split)...
2025-10-03 10:09:50,609 - INFO - Loaded 10 samples from The Pile
2025-10-03 10:09:50,609 - INFO - ================================================================================
2025-10-03 10:09:50,609 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 10:09:50,609 - INFO - ================================================================================
2025-10-03 10:09:50,609 - INFO - 
================================================================================
2025-10-03 10:09:50,609 - INFO - Evaluating: tinystories-33M
2025-10-03 10:09:50,609 - INFO - ================================================================================
2025-10-03 10:09:50,610 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 10:09:52,499 - INFO - Baseline         Using GPUs: [0, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 10:09:55,306 - INFO - tinystories-33M                 Baseline         PPL 4.68  Xent 1.5431  Entr 4.3353
2025-10-03 10:10:05,838 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 10:10:05,901 - INFO - MX-Selective     Using GPUs: [0, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 10:10:07,323 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 10:10:07,324 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 10:10:08,104 - INFO - tinystories-33M-MX              MX-Selective     PPL 4.70  Xent 1.5474  Entr 4.3424
2025-10-03 10:10:18,544 - INFO - 
================================================================================
2025-10-03 10:10:18,544 - INFO - BENCHMARK COMPLETE
2025-10-03 10:10:18,545 - INFO - ================================================================================
2025-10-03 10:18:07,578 - INFO - Will replace only MLP modules (Tier-A): ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 10:18:08,638 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 10:18:08,639 - INFO - Loading evaluation data...
2025-10-03 10:18:08,639 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-03 10:18:11,850 - INFO - Loaded 1000 samples from The Pile
2025-10-03 10:18:11,850 - INFO - ================================================================================
2025-10-03 10:18:11,850 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 10:18:11,850 - INFO - ================================================================================
2025-10-03 10:18:11,850 - INFO - 
================================================================================
2025-10-03 10:18:11,850 - INFO - Evaluating: llama-3.1-8b
2025-10-03 10:18:11,850 - INFO - ================================================================================
2025-10-03 10:18:11,851 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 10:18:13,783 - INFO - Baseline         Using GPUs: [0, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 10:20:22,045 - INFO - llama-3.1-8b                    Baseline         PPL 1.73  Xent 0.5484  Entr 1.5210
2025-10-03 10:20:44,127 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 10:20:44,182 - INFO - MX-Selective     Using GPUs: [0, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 10:20:45,819 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 10:20:45,822 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-03 10:23:17,393 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.75  Xent 0.5609  Entr 1.5969
2025-10-03 10:23:39,275 - INFO - 
================================================================================
2025-10-03 10:23:39,276 - INFO - Evaluating: qwen2.5-14b
2025-10-03 10:23:39,276 - INFO - ================================================================================
2025-10-03 10:23:39,276 - INFO - Using batch size override 1 for qwen2.5-14b
2025-10-03 10:23:39,276 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 10:23:39,282 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 10:23:49,465 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 10:28:12,828 - INFO - qwen2.5-14b                     Baseline         PPL 2.38  Xent 0.8682  Entr 2.2031
2025-10-03 10:28:23,304 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 10:28:23,306 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 10:28:34,548 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 10:28:34,550 - INFO - Applying MX quantization to Qwen/Qwen2.5-14B
2025-10-03 10:28:34,554 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-03 10:37:34,833 - INFO - qwen2.5-14b-MX                  MX-Selective     PPL 2.36  Xent 0.8566  Entr 2.1757
2025-10-03 10:37:45,250 - INFO - 
================================================================================
2025-10-03 10:37:45,250 - INFO - BENCHMARK COMPLETE
2025-10-03 10:37:45,250 - INFO - ================================================================================
2025-10-03 11:00:32,965 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 11:00:34,203 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 11:00:34,204 - INFO - Loading evaluation data...
2025-10-03 11:00:34,204 - INFO - Loading 10 samples from The Pile (train split)...
2025-10-03 11:00:38,636 - INFO - Loaded 10 samples from The Pile
2025-10-03 11:00:38,636 - INFO - ================================================================================
2025-10-03 11:00:38,638 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 11:00:38,638 - INFO - ================================================================================
2025-10-03 11:00:38,638 - INFO - 
================================================================================
2025-10-03 11:00:38,638 - INFO - Evaluating: tinystories-33M
2025-10-03 11:00:38,639 - INFO - ================================================================================
2025-10-03 11:00:38,639 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 11:00:40,494 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:00:43,028 - INFO - tinystories-33M                 Baseline         PPL 4.68  Xent 1.5431  Entr 4.3353
2025-10-03 11:00:53,505 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 11:00:53,551 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:00:55,006 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 11:00:55,007 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-03 11:00:55,790 - INFO - tinystories-33M-MX              MX-Selective     PPL 4.90  Xent 1.5889  Entr 4.4384
2025-10-03 11:01:06,217 - INFO - 
================================================================================
2025-10-03 11:01:06,217 - INFO - BENCHMARK COMPLETE
2025-10-03 11:01:06,217 - INFO - ================================================================================
2025-10-03 11:01:18,049 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 11:01:19,220 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 11:01:19,220 - INFO - Loading evaluation data...
2025-10-03 11:01:19,220 - INFO - Loading 5 samples from The Pile (train split)...
2025-10-03 11:01:22,269 - INFO - Loaded 5 samples from The Pile
2025-10-03 11:01:22,269 - INFO - ================================================================================
2025-10-03 11:01:22,269 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 11:01:22,269 - INFO - ================================================================================
2025-10-03 11:01:22,269 - INFO - 
================================================================================
2025-10-03 11:01:22,269 - INFO - Evaluating: tinystories-33M
2025-10-03 11:01:22,269 - INFO - ================================================================================
2025-10-03 11:01:22,269 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 11:01:24,040 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:01:26,509 - INFO - tinystories-33M                 Baseline         PPL 4.83  Xent 1.5758  Entr 4.4269
2025-10-03 11:01:36,984 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 11:01:37,033 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:01:38,526 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 11:01:38,527 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-03 11:01:39,242 - INFO - tinystories-33M-MX              MX-Selective     PPL 4.79  Xent 1.5671  Entr 4.3973
2025-10-03 11:01:49,670 - INFO - 
================================================================================
2025-10-03 11:01:49,671 - INFO - BENCHMARK COMPLETE
2025-10-03 11:01:49,671 - INFO - ================================================================================
2025-10-03 11:06:53,105 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 11:06:54,259 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 11:06:54,259 - INFO - Loading evaluation data...
2025-10-03 11:06:54,260 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-03 11:06:57,874 - INFO - Loaded 200 samples from The Pile
2025-10-03 11:06:57,875 - INFO - ================================================================================
2025-10-03 11:06:57,875 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 11:06:57,875 - INFO - ================================================================================
2025-10-03 11:06:57,875 - INFO - 
================================================================================
2025-10-03 11:06:57,875 - INFO - Evaluating: tinystories-33M
2025-10-03 11:06:57,875 - INFO - ================================================================================
2025-10-03 11:06:57,875 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 11:06:59,756 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:07:04,049 - INFO - tinystories-33M                 Baseline         PPL 5.16  Xent 1.6414  Entr 4.6259
2025-10-03 11:07:14,548 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 11:07:14,595 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:07:16,036 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 11:07:16,037 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-03 11:07:18,999 - INFO - tinystories-33M-MX              MX-Selective     PPL 5.28  Xent 1.6633  Entr 4.6771
2025-10-03 11:07:29,436 - INFO - 
================================================================================
2025-10-03 11:07:29,436 - INFO - Evaluating: phi-1_5
2025-10-03 11:07:29,436 - INFO - ================================================================================
2025-10-03 11:07:29,436 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 11:07:29,441 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:07:57,889 - INFO - phi-1_5                         Baseline         PPL 2.38  Xent 0.8663  Entr 2.4644
2025-10-03 11:08:12,804 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 11:08:12,808 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 11:08:14,359 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-03 11:08:14,363 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-03 11:08:42,937 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.43  Xent 0.8890  Entr 2.5634
2025-10-03 11:08:57,699 - INFO - 
================================================================================
2025-10-03 11:08:57,700 - INFO - Evaluating: llama-3.1-8b
2025-10-03 11:08:57,700 - INFO - ================================================================================
2025-10-03 11:08:57,700 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 11:08:57,701 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:19:34,916 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 13:19:36,134 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 13:19:36,135 - INFO - Loading evaluation data...
2025-10-03 13:19:36,135 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-03 13:19:40,499 - INFO - Loaded 200 samples from The Pile
2025-10-03 13:19:40,499 - INFO - ================================================================================
2025-10-03 13:19:40,501 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 13:19:40,501 - INFO - ================================================================================
2025-10-03 13:19:40,502 - INFO - 
================================================================================
2025-10-03 13:19:40,502 - INFO - Evaluating: tinystories-33M
2025-10-03 13:19:40,502 - INFO - ================================================================================
2025-10-03 13:19:40,502 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:19:42,389 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:19:47,051 - INFO - tinystories-33M                 Baseline         PPL 5.16  Xent 1.6414  Entr 4.6259
2025-10-03 13:19:57,559 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:19:57,603 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:19:59,014 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-03 13:19:59,015 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-03 13:20:02,069 - INFO - tinystories-33M-MX              MX-Selective     PPL 5.28  Xent 1.6633  Entr 4.6771
2025-10-03 13:20:12,513 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:20:12,518 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:20:12,814 - WARNING - Skipping INT8 benchmark for tinystories-33M: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:20:12,815 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:20:12,815 - INFO - 
================================================================================
2025-10-03 13:20:12,815 - INFO - Evaluating: phi-1_5
2025-10-03 13:20:12,815 - INFO - ================================================================================
2025-10-03 13:20:12,815 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:20:12,816 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:20:40,884 - INFO - phi-1_5                         Baseline         PPL 2.38  Xent 0.8663  Entr 2.4644
2025-10-03 13:20:55,554 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:20:55,556 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:20:57,112 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-03 13:20:57,116 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-03 13:21:25,693 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.43  Xent 0.8890  Entr 2.5634
2025-10-03 13:21:40,402 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:21:40,404 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:21:40,682 - WARNING - Skipping INT8 benchmark for phi-1_5: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:21:40,683 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:21:40,683 - INFO - 
================================================================================
2025-10-03 13:21:40,683 - INFO - Evaluating: llama-3.1-8b
2025-10-03 13:21:40,683 - INFO - ================================================================================
2025-10-03 13:21:40,683 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:21:40,684 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:23:42,281 - INFO - llama-3.1-8b                    Baseline         PPL 1.76  Xent 0.5670  Entr 1.5463
2025-10-03 13:24:04,097 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:24:04,098 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:24:05,765 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 13:24:05,770 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-03 13:24:45,065 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.75  Xent 0.5591  Entr 1.5309
2025-10-03 13:25:07,407 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:25:07,408 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:25:07,800 - WARNING - Skipping INT8 benchmark for llama-3.1-8b: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:25:07,800 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:25:07,800 - INFO - 
================================================================================
2025-10-03 13:25:07,800 - INFO - Evaluating: qwen2.5-7b
2025-10-03 13:25:07,801 - INFO - ================================================================================
2025-10-03 13:25:07,801 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:25:07,801 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:25:35,423 - INFO - qwen2.5-7b                      Baseline         PPL 2.87  Xent 1.0538  Entr 2.7963
2025-10-03 13:25:57,279 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:25:57,280 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:25:58,422 - INFO - Applying MX quantization to Qwen/Qwen2.5-7B
2025-10-03 13:25:58,427 - INFO - Replaced 196 Linear layers with MxLinear
2025-10-03 13:26:35,774 - INFO - qwen2.5-7b-MX                   MX-Selective     PPL 2.86  Xent 1.0524  Entr 2.7972
2025-10-03 13:26:57,514 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:26:57,515 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:26:57,802 - WARNING - Skipping INT8 benchmark for qwen2.5-7b: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:26:57,803 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:26:57,803 - INFO - 
================================================================================
2025-10-03 13:26:57,803 - INFO - Evaluating: phi-3-medium-14b
2025-10-03 13:26:57,803 - INFO - ================================================================================
2025-10-03 13:26:57,803 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:26:57,805 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:38:57,396 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 13:38:58,594 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 13:38:58,594 - INFO - Loading evaluation data...
2025-10-03 13:38:58,594 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-03 13:39:01,505 - INFO - Loaded 200 samples from The Pile
2025-10-03 13:39:01,506 - INFO - ================================================================================
2025-10-03 13:39:01,506 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 13:39:01,506 - INFO - ================================================================================
2025-10-03 13:39:01,506 - INFO - 
================================================================================
2025-10-03 13:39:01,506 - INFO - Evaluating: phi-1_5
2025-10-03 13:39:01,506 - INFO - ================================================================================
2025-10-03 13:39:01,506 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:39:03,277 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:40:00,350 - INFO - phi-1_5                         Baseline         PPL 2.38  Xent 0.8663  Entr 2.4644
2025-10-03 13:40:15,043 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:40:15,110 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:40:16,726 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-03 13:40:16,728 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-03 13:40:45,029 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.42  Xent 0.8851  Entr 2.5469
2025-10-03 13:40:59,748 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:40:59,751 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:41:00,141 - WARNING - Skipping INT8 benchmark for phi-1_5: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:41:00,141 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:41:00,142 - INFO - 
================================================================================
2025-10-03 13:41:00,142 - INFO - Evaluating: llama-3.1-8b
2025-10-03 13:41:00,142 - INFO - ================================================================================
2025-10-03 13:41:00,142 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:41:00,143 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:41:30,394 - INFO - llama-3.1-8b                    Baseline         PPL 1.76  Xent 0.5670  Entr 1.5463
2025-10-03 13:41:53,545 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:41:53,546 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:41:55,090 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 13:41:55,092 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-03 13:42:32,102 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.78  Xent 0.5755  Entr 1.6134
2025-10-03 13:42:55,250 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:42:55,252 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:42:55,644 - WARNING - Skipping INT8 benchmark for llama-3.1-8b: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:42:55,644 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:42:55,644 - INFO - 
================================================================================
2025-10-03 13:42:55,645 - INFO - Evaluating: qwen2.5-14b
2025-10-03 13:42:55,645 - INFO - ================================================================================
2025-10-03 13:42:55,645 - INFO - Using batch size override 1 for qwen2.5-14b
2025-10-03 13:42:55,645 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:42:55,646 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:43:04,212 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 13:43:57,692 - INFO - qwen2.5-14b                     Baseline         PPL 2.35  Xent 0.8533  Entr 2.1449
2025-10-03 13:44:08,121 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:44:08,123 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:44:17,438 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 13:44:17,440 - INFO - Applying MX quantization to Qwen/Qwen2.5-14B
2025-10-03 13:44:17,444 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-03 13:46:06,421 - INFO - qwen2.5-14b-MX                  MX-Selective     PPL 2.33  Xent 0.8438  Entr 2.1283
2025-10-03 13:46:16,833 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:46:16,835 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:46:18,084 - WARNING - Skipping INT8 benchmark for qwen2.5-14b: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:46:18,084 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:46:18,084 - INFO - 
================================================================================
2025-10-03 13:46:18,084 - INFO - BENCHMARK COMPLETE
2025-10-03 13:46:18,084 - INFO - ================================================================================
2025-10-03 13:53:55,628 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 13:53:55,628 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-03 13:53:56,899 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 13:53:56,902 - INFO - Loading evaluation data...
2025-10-03 13:53:56,902 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-03 13:54:16,233 - INFO - Loaded 200 samples from The Pile
2025-10-03 13:54:16,233 - INFO - ================================================================================
2025-10-03 13:54:16,233 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 13:54:16,233 - INFO - ================================================================================
2025-10-03 13:54:16,233 - INFO - 
================================================================================
2025-10-03 13:54:16,233 - INFO - Evaluating: llama-3.1-8b
2025-10-03 13:54:16,233 - INFO - ================================================================================
2025-10-03 13:54:16,233 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:54:18,144 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 14:02:41,526 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 14:02:41,526 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-03 14:02:42,640 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 14:02:42,640 - INFO - Loading evaluation data...
2025-10-03 14:02:42,640 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-03 14:02:46,084 - INFO - Loaded 200 samples from The Pile
2025-10-03 14:02:46,084 - INFO - ================================================================================
2025-10-03 14:02:46,084 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 14:02:46,085 - INFO - ================================================================================
2025-10-03 14:02:46,085 - INFO - 
================================================================================
2025-10-03 14:02:46,085 - INFO - Evaluating: llama-3.1-8b
2025-10-03 14:02:46,085 - INFO - ================================================================================
2025-10-03 14:02:46,085 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 14:02:47,983 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 14:03:17,844 - INFO - llama-3.1-8b                    Baseline         PPL 1.76  Xent 0.5670  Entr 1.5463
2025-10-03 14:03:40,690 - INFO - 
--- MX (Selective GEMM) --- Skipped (--skip_mx)
2025-10-03 14:03:40,690 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 14:03:40,721 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 14:04:24,482 - INFO - llama-3.1-8b-INT8               INT8             PPL 1.81  Xent 0.5951  Entr nan
2025-10-03 14:04:35,065 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 14:04:35,065 - INFO - 
================================================================================
2025-10-03 14:04:35,065 - INFO - Evaluating: qwen2.5-14b
2025-10-03 14:04:35,066 - INFO - ================================================================================
2025-10-03 14:04:35,066 - INFO - Using batch size override 1 for qwen2.5-14b
2025-10-03 14:04:35,066 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 14:04:35,066 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 14:04:44,393 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 14:05:37,152 - INFO - qwen2.5-14b                     Baseline         PPL 2.35  Xent 0.8533  Entr 2.1449
2025-10-03 14:05:47,613 - INFO - 
--- MX (Selective GEMM) --- Skipped (--skip_mx)
2025-10-03 14:05:47,614 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 14:05:47,617 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 14:07:37,957 - INFO - qwen2.5-14b-INT8                INT8             PPL 2.38  Xent 0.8671  Entr nan
2025-10-03 14:07:48,408 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 14:07:48,409 - INFO - 
================================================================================
2025-10-03 14:07:48,409 - INFO - BENCHMARK COMPLETE
2025-10-03 14:07:48,409 - INFO - ================================================================================
2025-10-06 13:40:13,880 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-06 13:40:13,880 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-06 13:40:14,292 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-06 13:40:14,292 - INFO - Loading evaluation data...
2025-10-06 13:40:14,292 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-06 13:40:18,956 - INFO - Loaded 200 samples from The Pile
2025-10-06 13:40:18,956 - INFO - ================================================================================
2025-10-06 13:40:18,956 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-06 13:40:18,956 - INFO - ================================================================================
2025-10-06 13:40:18,957 - INFO - 
================================================================================
2025-10-06 13:40:18,957 - INFO - Evaluating: deepseek-llm-67b-base
2025-10-06 13:40:18,957 - INFO - ================================================================================
2025-10-06 13:40:18,957 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-06 13:40:20,870 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:45:07,875 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-06 13:45:08,152 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-06 13:45:08,153 - INFO - Loading evaluation data...
2025-10-06 13:45:08,153 - INFO - Loading 20 samples from The Pile (train split)...
2025-10-06 13:45:11,719 - INFO - Loaded 20 samples from The Pile
2025-10-06 13:45:11,720 - INFO - ================================================================================
2025-10-06 13:45:11,720 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-06 13:45:11,720 - INFO - ================================================================================
2025-10-06 13:45:11,720 - INFO - 
================================================================================
2025-10-06 13:45:11,720 - INFO - Evaluating: tinystories-33M
2025-10-06 13:45:11,720 - INFO - ================================================================================
2025-10-06 13:45:11,720 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-06 13:45:13,808 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:45:16,362 - INFO - tinystories-33M                 Baseline         PPL 5.09  Xent 1.6269  Entr 4.5916
2025-10-06 13:45:26,872 - INFO - 
--- MX (Selective GEMM) ---
2025-10-06 13:45:26,905 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:45:28,294 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-06 13:45:28,295 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-06 13:46:06,714 - INFO - tinystories-33M-MX              MX-Selective     PPL 5.06  Xent 1.6221  Entr 4.6033
2025-10-06 13:46:17,156 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-06 13:46:17,159 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:46:20,708 - INFO - tinystories-33M-INT8            INT8             PPL 5.05  Xent 1.6186  Entr nan
2025-10-06 13:46:30,985 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-06 13:46:30,985 - INFO - 
================================================================================
2025-10-06 13:46:30,986 - INFO - BENCHMARK COMPLETE
2025-10-06 13:46:30,986 - INFO - ================================================================================
2025-10-06 13:54:19,838 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-06 13:54:20,121 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e5m2', 'a_elem_format_bp': 'fp8_e5m2', 'a_elem_format_bp_os': 'fp8_e5m2', 'a_elem_format_bp_ex': 'fp8_e5m2', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-06 13:54:20,121 - INFO - Loading evaluation data...
2025-10-06 13:54:20,121 - INFO - Loading 20 samples from The Pile (train split)...
2025-10-06 13:54:23,576 - INFO - Loaded 20 samples from The Pile
2025-10-06 13:54:23,577 - INFO - ================================================================================
2025-10-06 13:54:23,577 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-06 13:54:23,577 - INFO - ================================================================================
2025-10-06 13:54:23,577 - INFO - 
================================================================================
2025-10-06 13:54:23,577 - INFO - Evaluating: tinystories-33M
2025-10-06 13:54:23,577 - INFO - ================================================================================
2025-10-06 13:54:23,577 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-06 13:54:25,714 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:54:28,290 - INFO - tinystories-33M                 Baseline         PPL 5.09  Xent 1.6269  Entr 4.5916
2025-10-06 13:54:38,854 - INFO - 
--- MX (Selective GEMM) ---
2025-10-06 13:54:38,881 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:54:40,497 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-06 13:54:40,498 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-06 13:54:41,410 - INFO - tinystories-33M-MX              MX-Selective     PPL 4.90  Xent 1.5900  Entr 4.4749
2025-10-06 13:54:51,874 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-06 13:54:51,877 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:54:55,319 - INFO - tinystories-33M-INT8            INT8             PPL 5.05  Xent 1.6186  Entr nan
2025-10-06 13:55:05,627 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (transformer_engine not available)
2025-10-06 13:55:05,628 - INFO - 
================================================================================
2025-10-06 13:55:05,628 - INFO - BENCHMARK COMPLETE
2025-10-06 13:55:05,628 - INFO - ================================================================================
2025-10-06 13:55:27,322 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-06 13:55:27,323 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-06 13:55:27,605 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-06 13:55:27,605 - INFO - Loading evaluation data...
2025-10-06 13:55:27,605 - INFO - Loading 20 samples from The Pile (train split)...
2025-10-06 13:55:30,867 - INFO - Loaded 20 samples from The Pile
2025-10-06 13:55:30,867 - INFO - ================================================================================
2025-10-06 13:55:30,867 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-06 13:55:30,867 - INFO - ================================================================================
2025-10-06 13:55:30,867 - INFO - 
================================================================================
2025-10-06 13:55:30,867 - INFO - Evaluating: tinystories-33M
2025-10-06 13:55:30,867 - INFO - ================================================================================
2025-10-06 13:55:30,868 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-06 13:55:33,007 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:55:35,661 - INFO - tinystories-33M                 Baseline         PPL 5.09  Xent 1.6269  Entr 4.5916
2025-10-06 13:55:46,158 - INFO - 
--- MX (Selective GEMM) --- Skipped (--skip_mx)
2025-10-06 13:55:46,158 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-06 13:55:46,186 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-06 13:55:50,040 - INFO - tinystories-33M-INT8            INT8             PPL 5.05  Xent 1.6186  Entr nan
2025-10-06 13:56:00,429 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (transformer_engine not available)
2025-10-06 13:56:00,430 - INFO - 
================================================================================
2025-10-06 13:56:00,430 - INFO - BENCHMARK COMPLETE
2025-10-06 13:56:00,430 - INFO - ================================================================================
2025-10-07 20:07:56,910 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 20:07:56,912 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-07 20:07:56,912 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 20:07:56,912 - INFO - Loading evaluation data...
2025-10-07 20:07:56,912 - INFO - Loading 128 samples from The Pile (train split)...
2025-10-07 20:08:23,818 - INFO - Loaded 128 samples from The Pile
2025-10-07 20:08:23,818 - INFO - ================================================================================
2025-10-07 20:08:23,818 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 20:08:23,818 - INFO - ================================================================================
2025-10-07 20:08:23,818 - INFO - 
================================================================================
2025-10-07 20:08:23,818 - INFO - Evaluating: tinystories-33M
2025-10-07 20:08:23,818 - INFO - ================================================================================
2025-10-07 20:08:23,818 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 20:08:25,715 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 20:08:29,483 - INFO - tinystories-33M                 Baseline         PPL 5.08  Xent 1.6251  Entr 4.5819
2025-10-07 20:08:40,113 - INFO - 
--- MX (Selective GEMM) --- Skipped (--skip_mx)
2025-10-07 20:08:40,114 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 20:08:40,145 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 20:08:45,016 - INFO - tinystories-33M-INT8            INT8             PPL 5.12  Xent 1.6336  Entr nan
2025-10-07 20:08:55,366 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-07 20:08:55,367 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 20:08:56,878 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-07 20:08:57,420 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-07 20:08:57,421 - INFO - 
================================================================================
2025-10-07 20:08:57,421 - INFO - BENCHMARK COMPLETE
2025-10-07 20:08:57,421 - INFO - ================================================================================
2025-10-07 22:02:23,810 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:02:23,810 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-07 22:02:23,810 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:02:23,810 - INFO - Loading evaluation data...
2025-10-07 22:02:23,810 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:02:28,264 - INFO - Loaded 32 samples from The Pile
2025-10-07 22:02:28,264 - INFO - ================================================================================
2025-10-07 22:02:28,264 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:02:28,264 - INFO - ================================================================================
2025-10-07 22:02:28,264 - INFO - 
================================================================================
2025-10-07 22:02:28,264 - INFO - Evaluating: tinystories-33M
2025-10-07 22:02:28,264 - INFO - ================================================================================
2025-10-07 22:02:28,264 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:02:30,025 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:02:32,776 - INFO - tinystories-33M                 Baseline         PPL 4.72  Xent 1.5515  Entr 4.3962
2025-10-07 22:02:43,301 - INFO - 
--- MX (Selective GEMM) --- Skipped (--skip_mx)
2025-10-07 22:02:43,301 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 22:02:43,337 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:02:47,363 - INFO - tinystories-33M-INT8            INT8             PPL 4.72  Xent 1.5525  Entr nan
2025-10-07 22:02:57,703 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-07 22:02:57,704 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:02:59,315 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-07 22:02:59,756 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-07 22:02:59,756 - INFO - 
================================================================================
2025-10-07 22:02:59,757 - INFO - BENCHMARK COMPLETE
2025-10-07 22:02:59,757 - INFO - ================================================================================
2025-10-07 22:29:04,016 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:29:04,016 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-07 22:29:22,011 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:29:22,011 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-07 22:29:22,012 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:29:22,012 - INFO - Loading evaluation data...
2025-10-07 22:29:22,012 - INFO - Loading 1 samples from The Pile (train split)...
2025-10-07 22:29:25,180 - INFO - Loaded 1 samples from The Pile
2025-10-07 22:29:25,180 - INFO - ================================================================================
2025-10-07 22:29:25,180 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:29:25,180 - INFO - ================================================================================
2025-10-07 22:29:25,181 - INFO - 
================================================================================
2025-10-07 22:29:25,181 - INFO - Evaluating: tinystories-33M
2025-10-07 22:29:25,181 - INFO - ================================================================================
2025-10-07 22:29:25,181 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:29:26,934 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:29:58,016 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:29:58,016 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-07 22:29:58,018 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:29:58,018 - INFO - Loading evaluation data...
2025-10-07 22:29:58,019 - INFO - Loading 1 samples from The Pile (train split)...
2025-10-07 22:30:01,210 - INFO - Loaded 1 samples from The Pile
2025-10-07 22:30:01,210 - INFO - ================================================================================
2025-10-07 22:30:01,210 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:30:01,210 - INFO - ================================================================================
2025-10-07 22:30:01,210 - INFO - 
================================================================================
2025-10-07 22:30:01,210 - INFO - Evaluating: tinystories-33M
2025-10-07 22:30:01,210 - INFO - ================================================================================
2025-10-07 22:30:01,210 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:30:01,463 - INFO - Baseline         Using GPUs: [0] with 1 worker(s)/device
2025-10-07 22:32:46,806 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:32:46,806 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8 only.
2025-10-07 22:32:46,806 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:32:46,806 - INFO - Loading evaluation data...
2025-10-07 22:32:46,806 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:32:49,675 - INFO - Loaded 32 samples from The Pile
2025-10-07 22:32:49,675 - INFO - ================================================================================
2025-10-07 22:32:49,675 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:32:49,675 - INFO - ================================================================================
2025-10-07 22:32:49,675 - INFO - 
================================================================================
2025-10-07 22:32:49,675 - INFO - Evaluating: tinystories-33M
2025-10-07 22:32:49,675 - INFO - ================================================================================
2025-10-07 22:32:49,675 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:32:51,454 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:32:54,086 - INFO - tinystories-33M                 Baseline         PPL 4.72  Xent 1.5515  Entr 4.3962
2025-10-07 22:33:04,633 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-07 22:33:04,660 - INFO - BF16             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:33:06,470 - INFO - tinystories-33M-BF16            BF16             PPL 4.76  Xent 1.5605  Entr 4.4062
2025-10-07 22:33:16,857 - INFO - 
--- BFP16 (MX block-floating) ---
2025-10-07 22:33:16,857 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:33:16,859 - INFO - BFP16            Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:33:18,311 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-07 22:33:18,311 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-07 22:33:19,066 - INFO - tinystories-33M-BFP16           BFP16            PPL nan  Xent nan  Entr nan
2025-10-07 22:33:29,633 - INFO - 
--- NV-FP8 (Emulated, no shared exponent) ---
2025-10-07 22:33:29,634 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:33:29,634 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:33:29,636 - INFO - NVFP8-Emu        Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:33:31,045 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-07 22:33:31,046 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-07 22:33:31,761 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_sharedNVFP8-Emu        PPL 4.86  Xent 1.5801  Entr 4.4797
2025-10-07 22:33:32,738 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-07 22:33:32,738 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-07 22:33:33,685 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_sharedNVFP8-Emu        PPL 4.78  Xent 1.5651  Entr 4.4284
2025-10-07 22:33:42,338 - INFO - 
--- MX (Selective GEMM) --- Skipped (--skip_mx)
2025-10-07 22:33:42,338 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 22:33:42,340 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:33:56,211 - INFO - tinystories-33M-INT8            INT8             PPL 4.72  Xent 1.5525  Entr nan
2025-10-07 22:34:06,540 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-07 22:34:06,541 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:34:08,217 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-07 22:34:08,660 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-07 22:34:08,661 - INFO - 
================================================================================
2025-10-07 22:34:08,661 - INFO - BENCHMARK COMPLETE
2025-10-07 22:34:08,661 - INFO - ================================================================================
2025-10-07 22:45:36,851 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:45:36,851 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:45:36,851 - INFO - Loading evaluation data...
2025-10-07 22:45:36,851 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:46:28,268 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:46:28,269 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:46:28,269 - INFO - Loading evaluation data...
2025-10-07 22:46:28,269 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:46:31,615 - INFO - Loaded 32 samples from The Pile
2025-10-07 22:46:31,616 - INFO - ================================================================================
2025-10-07 22:46:31,616 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:46:31,616 - INFO - ================================================================================
2025-10-07 22:46:31,616 - INFO - 
================================================================================
2025-10-07 22:46:31,616 - INFO - Evaluating: phi-1_5
2025-10-07 22:46:31,616 - INFO - ================================================================================
2025-10-07 22:46:31,616 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:46:31,859 - WARNING - Baseline         No GPUs available. Using cpu.
2025-10-07 22:51:59,587 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:51:59,587 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:51:59,587 - INFO - Loading evaluation data...
2025-10-07 22:51:59,587 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:52:05,993 - INFO - Loaded 32 samples from The Pile
2025-10-07 22:52:05,993 - INFO - ================================================================================
2025-10-07 22:52:05,993 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:52:05,993 - INFO - ================================================================================
2025-10-07 22:52:05,993 - INFO - 
================================================================================
2025-10-07 22:52:05,994 - INFO - Evaluating: phi-1_5
2025-10-07 22:52:05,994 - INFO - ================================================================================
2025-10-07 22:52:05,994 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:52:08,153 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:52:15,715 - INFO - phi-1_5                         Baseline         PPL 2.43  Xent 0.8888  Entr 2.5461
2025-10-07 22:52:20,201 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-07 22:52:20,227 - INFO - BF16             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:52:23,772 - INFO - phi-1_5-BF16                    BF16             PPL 2.43  Xent 0.8882  Entr 2.5410
2025-10-07 22:52:25,985 - INFO - 
--- BFP16 (MX block-floating) ---
2025-10-07 22:52:25,986 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:52:25,987 - INFO - BFP16            Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:52:27,640 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:52:27,644 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:52:40,555 - INFO - phi-1_5-BFP16                   BFP16            PPL nan  Xent nan  Entr nan
2025-10-07 22:52:45,314 - INFO - 
--- NV-FP8 (Emulated, no shared exponent) ---
2025-10-07 22:52:45,315 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:52:45,316 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:52:45,317 - INFO - NVFP8-Emu        Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:52:46,991 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:52:46,996 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:52:48,832 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:52:48,836 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:53:02,150 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_sharedNVFP8-Emu        PPL 2.48  Xent 0.9080  Entr 2.6278
2025-10-07 22:53:03,306 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_sharedNVFP8-Emu        PPL 2.49  Xent 0.9133  Entr 2.6492
2025-10-07 22:53:08,788 - INFO - 
--- MX (Selective GEMM) ---
2025-10-07 22:53:08,790 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:53:10,684 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:53:10,687 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:53:17,340 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.50  Xent 0.9157  Entr 2.6521
2025-10-07 22:53:21,948 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 22:53:22,046 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:53:27,632 - INFO - phi-1_5-INT8                    INT8             PPL 2.43  Xent 0.8880  Entr nan
2025-10-07 22:53:27,935 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-07 22:53:27,936 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:53:31,395 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-07 22:53:35,662 - WARNING - Skipping NV-FP8 benchmark for phi-1_5: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-07 22:53:35,662 - INFO - 
================================================================================
2025-10-07 22:53:35,662 - INFO - BENCHMARK COMPLETE
2025-10-07 22:53:35,662 - INFO - ================================================================================
2025-10-07 22:54:10,751 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:54:10,751 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e5m2', 'a_elem_format_bp': 'fp8_e5m2', 'a_elem_format_bp_os': 'fp8_e5m2', 'a_elem_format_bp_ex': 'fp8_e5m2', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:54:10,751 - INFO - Loading evaluation data...
2025-10-07 22:54:10,751 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:54:13,914 - INFO - Loaded 32 samples from The Pile
2025-10-07 22:54:13,914 - INFO - ================================================================================
2025-10-07 22:54:13,914 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 22:54:13,914 - INFO - ================================================================================
2025-10-07 22:54:13,914 - INFO - 
================================================================================
2025-10-07 22:54:13,914 - INFO - Evaluating: phi-1_5
2025-10-07 22:54:13,914 - INFO - ================================================================================
2025-10-07 22:54:13,914 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 22:54:15,620 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:54:23,736 - INFO - phi-1_5                         Baseline         PPL 2.43  Xent 0.8888  Entr 2.5461
2025-10-07 22:54:38,735 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-07 22:54:38,759 - INFO - BF16             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:54:42,481 - INFO - phi-1_5-BF16                    BF16             PPL 2.43  Xent 0.8882  Entr 2.5410
2025-10-07 22:54:55,203 - INFO - 
--- BFP16 (MX block-floating) ---
2025-10-07 22:54:55,203 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:54:55,204 - INFO - BFP16            Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:54:56,975 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:54:56,980 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:55:09,682 - INFO - phi-1_5-BFP16                   BFP16            PPL nan  Xent nan  Entr nan
2025-10-07 22:55:24,635 - INFO - 
--- NV-FP8 (Emulated, no shared exponent) ---
2025-10-07 22:55:24,636 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:55:24,636 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 22:55:24,638 - INFO - NVFP8-Emu        Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:55:26,466 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:55:26,471 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:55:28,265 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:55:28,510 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:55:42,002 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_sharedNVFP8-Emu        PPL 2.48  Xent 0.9080  Entr 2.6278
2025-10-07 22:55:43,175 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_sharedNVFP8-Emu        PPL 2.49  Xent 0.9133  Entr 2.6492
2025-10-07 22:56:08,383 - INFO - 
--- MX (Selective GEMM) ---
2025-10-07 22:56:08,388 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:56:10,185 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 22:56:10,189 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 22:56:17,183 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.51  Xent 0.9191  Entr 2.6725
2025-10-07 22:56:32,275 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 22:56:32,278 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:56:38,581 - INFO - phi-1_5-INT8                    INT8             PPL 2.43  Xent 0.8880  Entr nan
2025-10-07 22:56:48,985 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-07 22:56:48,986 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 22:56:52,259 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-07 22:56:55,960 - WARNING - Skipping NV-FP8 benchmark for phi-1_5: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-07 22:56:55,960 - INFO - 
================================================================================
2025-10-07 22:56:55,960 - INFO - BENCHMARK COMPLETE
2025-10-07 22:56:55,960 - INFO - ================================================================================
2025-10-07 22:57:47,568 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 22:57:47,568 - INFO - MX Configuration (fp6_e3m2): {'w_elem_format': 'fp6_e3m2', 'a_elem_format': 'fp6_e3m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp6_e3m2', 'a_elem_format_bp': 'fp6_e3m2', 'a_elem_format_bp_os': 'fp6_e3m2', 'a_elem_format_bp_ex': 'fp6_e3m2', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 22:57:47,568 - INFO - Loading evaluation data...
2025-10-07 22:57:47,568 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 22:59:53,467 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:01:57,453 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 23:01:57,453 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 23:01:57,453 - INFO - Loading evaluation data...
2025-10-07 23:01:57,454 - INFO - Loading 16 samples from The Pile (train split)...
2025-10-07 23:02:01,928 - INFO - Loaded 16 samples from The Pile
2025-10-07 23:02:01,928 - INFO - ================================================================================
2025-10-07 23:02:01,928 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 23:02:01,928 - INFO - ================================================================================
2025-10-07 23:02:01,928 - INFO - 
================================================================================
2025-10-07 23:02:01,928 - INFO - Evaluating: phi-1_5
2025-10-07 23:02:01,928 - INFO - ================================================================================
2025-10-07 23:02:01,928 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 23:02:03,740 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:02:10,073 - INFO - phi-1_5                         Baseline         PPL 2.40  Xent 0.8769  Entr 2.4947
2025-10-07 23:02:24,770 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-07 23:02:24,837 - INFO - BF16             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:02:28,024 - INFO - phi-1_5-BF16                    BF16             PPL 2.40  Xent 0.8760  Entr 2.4883
2025-10-07 23:02:41,013 - INFO - 
--- BFP16 (MX block-floating) ---
2025-10-07 23:02:41,014 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 23:02:41,015 - INFO - BFP16            Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:02:42,804 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:02:42,808 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:02:47,703 - INFO - phi-1_5-BFP16                   BFP16            PPL 2.40  Xent 0.8770  Entr 2.4944
2025-10-07 23:03:02,645 - INFO - 
--- NV-FP8 (Emulated, no shared exponent) ---
2025-10-07 23:03:02,646 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 23:03:02,646 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 23:03:02,647 - INFO - NVFP8-Emu        Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:03:04,536 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:03:04,540 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:03:06,472 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:03:06,476 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:03:14,014 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_sharedNVFP8-Emu        PPL 2.46  Xent 0.9003  Entr 2.5843
2025-10-07 23:03:15,369 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_sharedNVFP8-Emu        PPL 2.47  Xent 0.9022  Entr 2.6010
2025-10-07 23:03:40,128 - INFO - 
--- MX (Selective GEMM) ---
2025-10-07 23:03:40,247 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:03:42,179 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:03:42,183 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:03:46,878 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.48  Xent 0.9080  Entr 2.6150
2025-10-07 23:04:01,645 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 23:04:01,718 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:04:06,891 - INFO - phi-1_5-INT8                    INT8             PPL 2.40  Xent 0.8770  Entr nan
2025-10-07 23:04:17,322 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-07 23:04:17,323 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:04:20,798 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-07 23:04:25,147 - WARNING - Skipping NV-FP8 benchmark for phi-1_5: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-07 23:04:25,147 - INFO - 
================================================================================
2025-10-07 23:04:25,147 - INFO - BENCHMARK COMPLETE
2025-10-07 23:04:25,148 - INFO - ================================================================================
2025-10-07 23:57:32,482 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-07 23:57:32,485 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-07 23:57:32,485 - INFO - Loading evaluation data...
2025-10-07 23:57:32,485 - INFO - Loading 32 samples from The Pile (train split)...
2025-10-07 23:57:37,136 - INFO - Loaded 32 samples from The Pile
2025-10-07 23:57:37,137 - INFO - ================================================================================
2025-10-07 23:57:37,137 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-07 23:57:37,137 - INFO - ================================================================================
2025-10-07 23:57:37,137 - INFO - 
================================================================================
2025-10-07 23:57:37,137 - INFO - Evaluating: phi-1_5
2025-10-07 23:57:37,137 - INFO - ================================================================================
2025-10-07 23:57:37,137 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-07 23:57:39,264 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:57:48,107 - INFO - phi-1_5                         Baseline         PPL 2.43  Xent 0.8888  Entr 2.5461
2025-10-07 23:58:03,000 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-07 23:58:03,032 - INFO - BF16             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:58:06,865 - INFO - phi-1_5-BF16                    BF16             PPL 2.43  Xent 0.8882  Entr 2.5410
2025-10-07 23:58:19,668 - INFO - 
--- BFP16 (MX block-floating) ---
2025-10-07 23:58:19,669 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 23:58:19,670 - INFO - BFP16            Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:58:21,747 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:58:21,751 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:58:28,001 - INFO - phi-1_5-BFP16                   BFP16            PPL 2.43  Xent 0.8888  Entr 2.5458
2025-10-07 23:58:43,442 - INFO - 
--- NV-FP8 (Emulated, no shared exponent) ---
2025-10-07 23:58:43,443 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 23:58:43,443 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-07 23:58:43,445 - INFO - NVFP8-Emu        Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:58:45,745 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:58:45,750 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:58:47,975 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:58:47,980 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:59:01,336 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_sharedNVFP8-Emu        PPL 2.48  Xent 0.9080  Entr 2.6278
2025-10-07 23:59:02,443 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_sharedNVFP8-Emu        PPL 2.49  Xent 0.9133  Entr 2.6492
2025-10-07 23:59:27,560 - INFO - 
--- MX (Selective GEMM) ---
2025-10-07 23:59:27,562 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:59:29,472 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-07 23:59:29,476 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-07 23:59:36,233 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.50  Xent 0.9157  Entr 2.6521
2025-10-07 23:59:51,513 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-07 23:59:51,517 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-07 23:59:57,904 - INFO - phi-1_5-INT8                    INT8             PPL 2.43  Xent 0.8880  Entr nan
2025-10-08 00:00:08,294 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-08 00:00:08,295 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 00:00:11,441 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-08 00:00:15,503 - WARNING - Skipping NV-FP8 benchmark for phi-1_5: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-08 00:00:15,503 - INFO - 
================================================================================
2025-10-08 00:00:15,503 - INFO - BENCHMARK COMPLETE
2025-10-08 00:00:15,504 - INFO - ================================================================================
2025-10-08 13:13:55,312 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-08 13:13:55,312 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:13:55,312 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:13:55,313 - INFO - MX Configuration (fp6_e3m2): {'w_elem_format': 'fp6_e3m2', 'a_elem_format': 'fp6_e3m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:13:55,313 - INFO - MX Configuration (fp6_e2m3): {'w_elem_format': 'fp6_e2m3', 'a_elem_format': 'fp6_e2m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:13:55,313 - INFO - MX Configuration (fp4_e2m1): {'w_elem_format': 'fp4_e2m1', 'a_elem_format': 'fp4_e2m1', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:13:55,313 - INFO - Loading evaluation data...
2025-10-08 13:13:55,313 - INFO - Loading 1 samples from The Pile (train split)...
2025-10-08 13:13:59,803 - INFO - Loaded 1 samples from The Pile
2025-10-08 13:13:59,803 - INFO - ================================================================================
2025-10-08 13:13:59,803 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-08 13:13:59,803 - INFO - ================================================================================
2025-10-08 13:13:59,803 - INFO - 
================================================================================
2025-10-08 13:13:59,804 - INFO - Evaluating: tinystories-33M
2025-10-08 13:13:59,804 - INFO - ================================================================================
2025-10-08 13:13:59,804 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-08 13:14:03,796 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:14:06,284 - INFO - tinystories-33M                 Baseline         PPL 4.83  Xent 1.5753  Entr 4.3854
2025-10-08 13:14:06,753 - INFO - 
--- MX (Selective GEMM) ---
2025-10-08 13:14:06,771 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:14:08,124 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-08 13:14:08,125 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-08 13:14:08,710 - INFO - tinystories-33M-MX-fp8_e4m3     MX-Selective     PPL 4.97  Xent 1.6041  Entr 4.4461
2025-10-08 13:14:09,912 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-08 13:14:09,912 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-08 13:14:10,086 - INFO - tinystories-33M-MX-fp8_e5m2     MX-Selective     PPL 4.97  Xent 1.6027  Entr 4.3582
2025-10-08 13:14:11,669 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-08 13:14:11,670 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-08 13:14:11,832 - INFO - tinystories-33M-MX-fp6_e3m2     MX-Selective     PPL 4.98  Xent 1.6063  Entr 4.3478
2025-10-08 13:14:13,242 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-08 13:14:13,242 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-08 13:14:14,206 - INFO - tinystories-33M-MX-fp6_e2m3     MX-Selective     PPL 4.84  Xent 1.5770  Entr 4.4084
2025-10-08 13:14:14,856 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-08 13:14:14,857 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-08 13:14:15,019 - INFO - tinystories-33M-MX-fp4_e2m1     MX-Selective     PPL 4.74  Xent 1.5567  Entr 4.2623
2025-10-08 13:14:15,441 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-08 13:14:15,442 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:14:17,995 - INFO - tinystories-33M-INT8            INT8             PPL 4.94  Xent 1.5977  Entr nan
2025-10-08 13:14:18,275 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-08 13:14:18,276 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:14:20,139 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-08 13:14:20,560 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-08 13:14:20,560 - INFO - 
================================================================================
2025-10-08 13:14:20,561 - INFO - BENCHMARK COMPLETE
2025-10-08 13:14:20,561 - INFO - ================================================================================
2025-10-08 13:18:49,892 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-08 13:18:49,892 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:18:49,892 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:18:49,892 - INFO - MX Configuration (fp6_e3m2): {'w_elem_format': 'fp6_e3m2', 'a_elem_format': 'fp6_e3m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:18:49,893 - INFO - MX Configuration (fp6_e2m3): {'w_elem_format': 'fp6_e2m3', 'a_elem_format': 'fp6_e2m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:18:49,893 - INFO - MX Configuration (fp4_e2m1): {'w_elem_format': 'fp4_e2m1', 'a_elem_format': 'fp4_e2m1', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:18:49,893 - INFO - Loading evaluation data...
2025-10-08 13:18:49,893 - INFO - Loading 128 samples from The Pile (train split)...
2025-10-08 13:18:53,040 - INFO - Loaded 128 samples from The Pile
2025-10-08 13:18:53,040 - INFO - ================================================================================
2025-10-08 13:18:53,040 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-08 13:18:53,040 - INFO - ================================================================================
2025-10-08 13:18:53,040 - INFO - 
================================================================================
2025-10-08 13:18:53,040 - INFO - Evaluating: llama-3.1-8b
2025-10-08 13:18:53,041 - INFO - ================================================================================
2025-10-08 13:18:53,041 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-08 13:18:56,097 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:19:31,639 - INFO - llama-3.1-8b                    Baseline         PPL 1.75  Xent 0.5569  Entr 1.5557
2025-10-08 13:19:54,442 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-08 13:19:54,491 - INFO - BF16             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:20:15,787 - INFO - llama-3.1-8b-BF16               BF16             PPL 1.75  Xent 0.5569  Entr 1.5557
2025-10-08 13:20:37,934 - INFO - 
--- BFP16 (MX block-floating) ---
2025-10-08 13:20:37,935 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-08 13:20:39,067 - INFO - BFP16            Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:20:40,714 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:20:40,720 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:21:01,448 - INFO - llama-3.1-8b-BFP16              BFP16            PPL 1.75  Xent 0.5569  Entr 1.5557
2025-10-08 13:21:25,550 - INFO - 
--- NV-FP8 (Emulated, no shared exponent) ---
2025-10-08 13:21:25,551 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-08 13:21:25,551 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-08 13:21:25,552 - INFO - NVFP8-Emu        Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:21:27,173 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:21:27,179 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:21:28,762 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:21:28,768 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:23:28,471 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_sharedNVFP8-Emu        PPL 1.69  Xent 0.5275  Entr 1.4517
2025-10-08 13:23:33,259 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_sharedNVFP8-Emu        PPL 1.74  Xent 0.5539  Entr 1.5508
2025-10-08 13:24:02,093 - INFO - 
--- MX (Selective GEMM) ---
2025-10-08 13:24:02,094 - INFO - MX-Selective     Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:24:03,744 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:24:03,750 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:24:05,325 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:24:05,330 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:24:07,222 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:24:07,227 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:24:08,995 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:24:09,001 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:24:10,748 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-08 13:24:10,753 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-08 13:24:45,254 - INFO - llama-3.1-8b-MX-fp8_e4m3        MX-Selective     PPL 1.73  Xent 0.5476  Entr 1.5364
2025-10-08 13:24:45,922 - INFO - llama-3.1-8b-MX-fp8_e5m2        MX-Selective     PPL 1.77  Xent 0.5710  Entr 1.6145
2025-10-08 13:24:47,822 - INFO - llama-3.1-8b-MX-fp6_e3m2        MX-Selective     PPL 1.77  Xent 0.5689  Entr 1.6064
2025-10-08 13:24:49,416 - INFO - llama-3.1-8b-MX-fp6_e2m3        MX-Selective     PPL 1.72  Xent 0.5396  Entr 1.4893
2025-10-08 13:24:49,707 - INFO - llama-3.1-8b-MX-fp4_e2m1        MX-Selective     PPL 1.99  Xent 0.6860  Entr 2.0876
2025-10-08 13:25:54,384 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-08 13:25:54,388 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:26:25,758 - INFO - llama-3.1-8b-INT8               INT8             PPL 1.80  Xent 0.5885  Entr nan
2025-10-08 13:26:36,229 - INFO - 
--- NV-FP8 (Transformer Engine) ---
2025-10-08 13:26:36,685 - INFO - NV-FP8           Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:26:41,664 - INFO - Replaced 224 Linear layers with NvFp8Linear
2025-10-08 13:26:53,011 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-08 13:26:53,011 - INFO - 
================================================================================
2025-10-08 13:26:53,011 - INFO - BENCHMARK COMPLETE
2025-10-08 13:26:53,012 - INFO - ================================================================================
2025-10-08 13:38:40,086 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-08 13:38:40,086 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:38:40,087 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:38:40,087 - INFO - MX Configuration (fp6_e3m2): {'w_elem_format': 'fp6_e3m2', 'a_elem_format': 'fp6_e3m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:38:40,087 - INFO - MX Configuration (fp6_e2m3): {'w_elem_format': 'fp6_e2m3', 'a_elem_format': 'fp6_e2m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:38:40,087 - INFO - MX Configuration (fp4_e2m1): {'w_elem_format': 'fp4_e2m1', 'a_elem_format': 'fp4_e2m1', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:38:40,087 - INFO - Loading evaluation data...
2025-10-08 13:38:40,087 - INFO - Loading 128 samples from The Pile (train split)...
2025-10-08 13:38:43,543 - INFO - Loaded 128 samples from The Pile
2025-10-08 13:38:43,544 - INFO - ================================================================================
2025-10-08 13:38:43,544 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-08 13:38:43,544 - INFO - ================================================================================
2025-10-08 13:38:43,544 - INFO - 
================================================================================
2025-10-08 13:38:43,544 - INFO - Evaluating: deepseek-llm-67b-base
2025-10-08 13:38:43,544 - INFO - ================================================================================
2025-10-08 13:38:43,544 - INFO - Using batch size override 1 for deepseek-llm-67b-base
2025-10-08 13:38:43,544 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-08 13:38:47,045 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 13:54:39,646 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-08 13:54:39,646 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:54:39,646 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:54:39,646 - INFO - MX Configuration (fp6_e3m2): {'w_elem_format': 'fp6_e3m2', 'a_elem_format': 'fp6_e3m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:54:39,646 - INFO - MX Configuration (fp6_e2m3): {'w_elem_format': 'fp6_e2m3', 'a_elem_format': 'fp6_e2m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:54:39,647 - INFO - MX Configuration (fp4_e2m1): {'w_elem_format': 'fp4_e2m1', 'a_elem_format': 'fp4_e2m1', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 13:54:39,647 - INFO - Loading evaluation data...
2025-10-08 13:54:39,647 - INFO - Loading 128 samples from The Pile (train split)...
2025-10-08 13:54:43,950 - INFO - Loaded 128 samples from The Pile
2025-10-08 13:54:43,950 - INFO - ================================================================================
2025-10-08 13:54:43,950 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-08 13:54:43,950 - INFO - ================================================================================
2025-10-08 13:54:43,950 - INFO - 
================================================================================
2025-10-08 13:54:43,950 - INFO - Evaluating: deepseek-llm-67b-base
2025-10-08 13:54:43,950 - INFO - ================================================================================
2025-10-08 13:54:43,950 - INFO - Using batch size override 1 for deepseek-llm-67b-base
2025-10-08 13:54:43,950 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-08 13:54:45,663 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-08 16:36:24,627 - INFO - Tier B selected. Will replace modules matching: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'c_attn', 'c_proj', 'query', 'key', 'value', 'dense', 'Wqkv', 'out_proj', 'gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-08 16:36:24,627 - INFO - MX Configuration (fp8_e4m3): {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 16:36:24,627 - INFO - MX Configuration (fp8_e5m2): {'w_elem_format': 'fp8_e5m2', 'a_elem_format': 'fp8_e5m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 16:36:24,627 - INFO - MX Configuration (fp6_e3m2): {'w_elem_format': 'fp6_e3m2', 'a_elem_format': 'fp6_e3m2', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 16:36:24,627 - INFO - MX Configuration (fp6_e2m3): {'w_elem_format': 'fp6_e2m3', 'a_elem_format': 'fp6_e2m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 16:36:24,627 - INFO - MX Configuration (fp4_e2m1): {'w_elem_format': 'fp4_e2m1', 'a_elem_format': 'fp4_e2m1', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even'}
2025-10-08 16:36:24,628 - INFO - Loading evaluation data...
2025-10-08 16:36:24,628 - INFO - Loading 128 samples from The Pile (train split)...
2025-10-08 16:36:29,602 - INFO - Loaded 128 samples from The Pile
2025-10-08 16:36:29,603 - INFO - ================================================================================
2025-10-08 16:36:29,603 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-08 16:36:29,603 - INFO - ================================================================================
2025-10-08 16:36:29,603 - INFO - 
================================================================================
2025-10-08 16:36:29,603 - INFO - Evaluating: deepseek-llm-67b-base
2025-10-08 16:36:29,603 - INFO - ================================================================================
2025-10-08 16:36:29,603 - INFO - Using batch size override 1 for deepseek-llm-67b-base
2025-10-08 16:36:29,603 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-08 16:36:31,305 - INFO - Baseline         Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:11:05,049 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:11:05,050 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:11:05,050 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:11:05,050 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:11:05,050 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:11:05,050 - INFO - Loading evaluation data...
2025-10-10 13:11:05,050 - INFO - Loading 20000 samples from The Pile (validation split)...
2025-10-10 13:11:07,675 - WARNING - Could not load The Pile: Bad split: validation. Available splits: ['train']. Falling back to WikiText.
2025-10-10 13:11:10,455 - INFO - ================================================================================
2025-10-10 13:11:10,455 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:11:10,456 - INFO - ================================================================================
2025-10-10 13:11:10,456 - INFO - 
================================================================================
2025-10-10 13:11:10,456 - INFO - Evaluating: tinystories-33M
2025-10-10 13:11:10,456 - INFO - ================================================================================
2025-10-10 13:11:10,456 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:11:12,381 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:13:44,700 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:13:44,700 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:13:44,700 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:13:44,700 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:13:44,700 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:13:44,700 - INFO - Loading evaluation data...
2025-10-10 13:13:44,700 - INFO - Loading 20000 samples from The Pile (train split)...
2025-10-10 13:13:51,226 - INFO - Loaded 20000 samples from The Pile
2025-10-10 13:13:51,226 - INFO - ================================================================================
2025-10-10 13:13:51,226 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:13:51,226 - INFO - ================================================================================
2025-10-10 13:13:51,226 - INFO - 
================================================================================
2025-10-10 13:13:51,226 - INFO - Evaluating: tinystories-33M
2025-10-10 13:13:51,226 - INFO - ================================================================================
2025-10-10 13:13:51,226 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:13:53,199 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:16:52,605 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:16:52,605 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:16:52,605 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:16:52,605 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:16:52,605 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:16:52,606 - INFO - Loading evaluation data...
2025-10-10 13:16:52,606 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-10 13:16:55,404 - INFO - Loaded 1000 samples from The Pile
2025-10-10 13:16:55,404 - INFO - ================================================================================
2025-10-10 13:16:55,404 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:16:55,404 - INFO - ================================================================================
2025-10-10 13:16:55,404 - INFO - 
================================================================================
2025-10-10 13:16:55,404 - INFO - Evaluating: tinystories-33M
2025-10-10 13:16:55,404 - INFO - ================================================================================
2025-10-10 13:16:55,404 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:16:57,136 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:17:10,604 - INFO - tinystories-33M                 Baseline         PPL 5.09  Xent 1.6270  Entr 4.6149
2025-10-10 13:17:42,157 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:17:42,157 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:17:42,158 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:17:42,158 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:17:42,158 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:17:42,158 - INFO - Loading evaluation data...
2025-10-10 13:17:42,158 - INFO - Loading 10005000 samples from The Pile (train split)...
2025-10-10 13:18:32,094 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:18:32,094 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:18:32,094 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:18:32,094 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:18:32,094 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:18:32,094 - INFO - Loading evaluation data...
2025-10-10 13:18:32,094 - INFO - Loading 10000 samples from The Pile (train split)...
2025-10-10 13:18:34,894 - INFO - Loaded 10000 samples from The Pile
2025-10-10 13:18:34,894 - INFO - ================================================================================
2025-10-10 13:18:34,894 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:18:34,894 - INFO - ================================================================================
2025-10-10 13:18:34,894 - INFO - 
================================================================================
2025-10-10 13:18:34,894 - INFO - Evaluating: tinystories-33M
2025-10-10 13:18:34,894 - INFO - ================================================================================
2025-10-10 13:18:34,895 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:18:36,649 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:20:21,486 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:20:21,486 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:20:21,486 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:20:21,486 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:20:21,486 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:20:21,486 - INFO - Loading evaluation data...
2025-10-10 13:20:21,486 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-10 13:20:23,220 - INFO - Loaded 1000 samples from The Pile
2025-10-10 13:20:23,220 - INFO - ================================================================================
2025-10-10 13:20:23,220 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:20:23,220 - INFO - ================================================================================
2025-10-10 13:20:23,220 - INFO - 
================================================================================
2025-10-10 13:20:23,221 - INFO - Evaluating: tinystories-33M
2025-10-10 13:20:23,221 - INFO - ================================================================================
2025-10-10 13:20:23,221 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:20:25,009 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:20:39,039 - INFO - tinystories-33M                 Baseline         PPL 5.03  Xent 1.6149  Entr 4.5666
2025-10-10 13:21:03,922 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:21:03,922 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:21:03,922 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:21:03,922 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:21:03,922 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:21:03,922 - INFO - Loading evaluation data...
2025-10-10 13:21:03,922 - INFO - Loading 1050 samples from The Pile (train split)...
2025-10-10 13:21:05,710 - INFO - Loaded 1050 samples from The Pile
2025-10-10 13:21:05,710 - INFO - ================================================================================
2025-10-10 13:21:05,710 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:21:05,711 - INFO - ================================================================================
2025-10-10 13:21:05,711 - INFO - 
================================================================================
2025-10-10 13:21:05,711 - INFO - Evaluating: tinystories-33M
2025-10-10 13:21:05,711 - INFO - ================================================================================
2025-10-10 13:21:05,711 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:21:07,848 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:22:27,442 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:22:27,443 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:22:27,443 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:22:27,443 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:22:27,443 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:22:27,443 - INFO - Loading evaluation data...
2025-10-10 13:22:27,443 - INFO - Loading 5000 samples from The Pile (train split)...
2025-10-10 13:22:29,617 - INFO - Loaded 5000 samples from The Pile
2025-10-10 13:22:29,618 - INFO - ================================================================================
2025-10-10 13:22:29,618 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:22:29,618 - INFO - ================================================================================
2025-10-10 13:22:29,618 - INFO - 
================================================================================
2025-10-10 13:22:29,618 - INFO - Evaluating: tinystories-33M
2025-10-10 13:22:29,618 - INFO - ================================================================================
2025-10-10 13:22:29,618 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:22:31,581 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:23:29,784 - INFO - tinystories-33M                 Baseline         PPL 5.12  Xent 1.6334  Entr 4.6108
2025-10-10 13:23:50,381 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-10 13:23:50,450 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:24:24,853 - INFO - tinystories-33M-BF16            BF16             PPL 5.15  Xent 1.6395  Entr 4.6254
2025-10-10 13:24:45,305 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-10-10 13:24:45,306 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:24:45,308 - INFO - BFP16-TA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:24:46,633 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:24:46,633 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:25:43,944 - INFO - tinystories-33M-BFP16-TA        BFP16-TA         PPL 5.12  Xent 1.6338  Entr 4.6117
2025-10-10 13:26:04,550 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:26:04,551 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:26:04,553 - INFO - NVFP8Emu-TA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:26:05,886 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:26:05,887 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:26:07,417 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:26:07,417 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:27:45,392 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TANVFP8Emu-TA      PPL 5.13  Xent 1.6356  Entr 4.6138
2025-10-10 13:27:46,164 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TANVFP8Emu-TA      PPL 5.31  Xent 1.6689  Entr 4.6978
2025-10-10 13:28:06,009 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:28:06,010 - INFO - MX Configuration (fp8_e5m2): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:28:06,011 - INFO - MX Configuration (fp6_e3m2): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e3m2",
    "a_elem_format": "fp6_e3m2",
    "w_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp_ex": "fp6_e3m2",
    "a_elem_format_bp_os": "fp6_e3m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:28:06,011 - INFO - MX Configuration (fp6_e2m3): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e2m3",
    "a_elem_format": "fp6_e2m3",
    "w_elem_format_bp": "fp6_e2m3",
    "a_elem_format_bp": "fp6_e2m3",
    "a_elem_format_bp_ex": "fp6_e2m3",
    "a_elem_format_bp_os": "fp6_e2m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:28:06,011 - INFO - MX Configuration (fp4_e2m1): {
    "scale_bits": 8,
    "w_elem_format": "fp4_e2m1",
    "a_elem_format": "fp4_e2m1",
    "w_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp_ex": "fp4_e2m1",
    "a_elem_format_bp_os": "fp4_e2m1",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:28:06,013 - INFO - MX-TA            Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:28:07,386 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:28:07,387 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:28:08,942 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:28:08,942 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:28:10,588 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:28:10,591 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:28:12,177 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:28:12,182 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:28:13,770 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:28:13,771 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:29:33,432 - INFO - tinystories-33M-TierA-fp6_e2m3  MX-TA            PPL 5.17  Xent 1.6427  Entr 4.6273
2025-10-10 13:29:33,612 - INFO - tinystories-33M-TierA-fp6_e3m2  MX-TA            PPL 5.12  Xent 1.6336  Entr 4.6117
2025-10-10 13:29:34,421 - INFO - tinystories-33M-TierA-fp8_e4m3  MX-TA            PPL 5.23  Xent 1.6549  Entr 4.6628
2025-10-10 13:29:34,650 - INFO - tinystories-33M-TierA-fp4_e2m1  MX-TA            PPL 5.41  Xent 1.6878  Entr 4.7698
2025-10-10 13:29:44,657 - INFO - tinystories-33M-TierA-fp8_e5m2  MX-TA            PPL 5.12  Xent 1.6337  Entr 4.6121
2025-10-10 13:30:36,131 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:30:36,132 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:30:36,132 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:30:36,132 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:30:36,132 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:30:36,132 - INFO - Loading evaluation data...
2025-10-10 13:30:36,132 - INFO - Loading 50001 samples from The Pile (train split)...
2025-10-10 13:30:45,335 - INFO - Loaded 50001 samples from The Pile
2025-10-10 13:30:45,335 - INFO - ================================================================================
2025-10-10 13:30:45,336 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:30:45,336 - INFO - ================================================================================
2025-10-10 13:30:45,336 - INFO - 
================================================================================
2025-10-10 13:30:45,336 - INFO - Evaluating: tinystories-33M
2025-10-10 13:30:45,336 - INFO - ================================================================================
2025-10-10 13:30:45,336 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:31:43,708 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:31:43,709 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:31:43,709 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:31:43,709 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:31:43,709 - INFO - Tier E selected: MLP + attention projections in MXFP4 (most aggressive)
2025-10-10 13:31:43,709 - INFO - Loading evaluation data...
2025-10-10 13:31:43,709 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-10 13:31:45,492 - INFO - Loaded 1000 samples from The Pile
2025-10-10 13:31:45,492 - INFO - ================================================================================
2025-10-10 13:31:45,492 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:31:45,492 - INFO - ================================================================================
2025-10-10 13:31:45,493 - INFO - 
================================================================================
2025-10-10 13:31:45,493 - INFO - Evaluating: tinystories-33M
2025-10-10 13:31:45,493 - INFO - ================================================================================
2025-10-10 13:31:45,493 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:31:47,434 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:32:01,941 - INFO - tinystories-33M                 Baseline         PPL 5.03  Xent 1.6149  Entr 4.5666
2025-10-10 13:32:22,461 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-10 13:32:22,490 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:32:31,356 - INFO - tinystories-33M-BF16            BF16             PPL 5.05  Xent 1.6202  Entr 4.5811
2025-10-10 13:32:51,786 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-10-10 13:32:51,787 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:32:51,788 - INFO - BFP16-TA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:32:53,143 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:32:53,143 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:33:05,293 - INFO - tinystories-33M-BFP16-TA        BFP16-TA         PPL 5.03  Xent 1.6153  Entr 4.5674
2025-10-10 13:33:25,839 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:33:25,840 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:33:25,841 - INFO - NVFP8Emu-TA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:33:27,208 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:33:27,209 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:33:29,158 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:33:29,160 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:33:47,233 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TANVFP8Emu-TA      PPL 5.21  Xent 1.6509  Entr 4.6592
2025-10-10 13:33:48,847 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TANVFP8Emu-TA      PPL 5.03  Xent 1.6157  Entr 4.5695
2025-10-10 13:34:07,838 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:34:07,839 - INFO - MX Configuration (fp8_e5m2): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:34:07,839 - INFO - MX Configuration (fp6_e3m2): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e3m2",
    "a_elem_format": "fp6_e3m2",
    "w_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp_ex": "fp6_e3m2",
    "a_elem_format_bp_os": "fp6_e3m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:34:07,840 - INFO - MX Configuration (fp6_e2m3): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e2m3",
    "a_elem_format": "fp6_e2m3",
    "w_elem_format_bp": "fp6_e2m3",
    "a_elem_format_bp": "fp6_e2m3",
    "a_elem_format_bp_ex": "fp6_e2m3",
    "a_elem_format_bp_os": "fp6_e2m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:34:07,840 - INFO - MX Configuration (fp4_e2m1): {
    "scale_bits": 8,
    "w_elem_format": "fp4_e2m1",
    "a_elem_format": "fp4_e2m1",
    "w_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp_ex": "fp4_e2m1",
    "a_elem_format_bp_os": "fp4_e2m1",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:34:07,841 - INFO - MX-TA            Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:34:09,242 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:34:09,243 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:34:11,291 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:34:11,292 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:34:13,033 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:34:13,035 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:34:15,134 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:34:15,146 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:34:18,156 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:34:18,159 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:34:42,903 - INFO - tinystories-33M-TierA-fp8_e5m2  MX-TA            PPL 5.02  Xent 1.6140  Entr 4.5646
2025-10-10 13:34:43,023 - INFO - tinystories-33M-TierA-fp8_e4m3  MX-TA            PPL 5.15  Xent 1.6381  Entr 4.6228
2025-10-10 13:34:43,117 - INFO - tinystories-33M-TierA-fp4_e2m1  MX-TA            PPL 5.31  Xent 1.6703  Entr 4.7302
2025-10-10 13:34:43,151 - INFO - tinystories-33M-TierA-fp6_e3m2  MX-TA            PPL 5.03  Xent 1.6144  Entr 4.5646
2025-10-10 13:34:43,245 - INFO - tinystories-33M-TierA-fp6_e2m3  MX-TA            PPL 5.08  Xent 1.6246  Entr 4.5827
2025-10-10 13:39:21,752 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:39:21,753 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:39:21,753 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:39:21,753 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:39:21,753 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8/BFP16 only.
2025-10-10 13:39:21,753 - INFO - Loading evaluation data...
2025-10-10 13:39:21,753 - INFO - Loading 1 samples from The Pile (train split)...
2025-10-10 13:39:23,884 - INFO - Loaded 1 samples from The Pile
2025-10-10 13:39:23,885 - INFO - ================================================================================
2025-10-10 13:39:23,885 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:39:23,885 - INFO - ================================================================================
2025-10-10 13:39:23,885 - INFO - 
================================================================================
2025-10-10 13:39:23,885 - INFO - Evaluating: tinystories-33M
2025-10-10 13:39:23,885 - INFO - ================================================================================
2025-10-10 13:39:23,885 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:39:25,687 - WARNING - Baseline         No GPUs available. Using cpu.
2025-10-10 13:39:27,819 - INFO - tinystories-33M                 Baseline         PPL 8.59  Xent 2.1503  Entr 5.8763
2025-10-10 13:39:27,999 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-10-10 13:39:27,999 - INFO - --- MX (Selective GEMM) [TA] --- Skipped (--skip_mx)
2025-10-10 13:39:28,021 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TA]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:39:28,021 - INFO - 
--- Tier TB: MLP + attention projections in MXFP8 (safe) ---
2025-10-10 13:39:28,021 - INFO - --- MX (Selective GEMM) [TB] --- Skipped (--skip_mx)
2025-10-10 13:39:28,021 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TB]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:39:28,021 - INFO - 
--- Tier TC: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-10 13:39:28,021 - INFO - --- MX (Selective GEMM) [TC] --- Skipped (--skip_mx)
2025-10-10 13:39:28,022 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TC]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:39:28,022 - INFO - 
--- Tier TD: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-10 13:39:28,022 - INFO - --- MX (Selective GEMM) [TD] --- Skipped (--skip_mx)
2025-10-10 13:39:28,022 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TD]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:39:28,022 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-10 13:39:28,022 - WARNING - Skipping INT8 benchmark for tinystories-33M: No eligible CUDA devices for INT8 backend
2025-10-10 13:39:28,023 - INFO - 
================================================================================
2025-10-10 13:39:28,023 - INFO - BENCHMARK COMPLETE
2025-10-10 13:39:28,023 - INFO - ================================================================================
2025-10-10 13:40:26,188 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:40:26,188 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:40:26,188 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:40:26,188 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:40:26,188 - INFO - MX stage skipped by flag; tier selection will apply to NV-FP8/BFP16 only.
2025-10-10 13:40:26,188 - INFO - Loading evaluation data...
2025-10-10 13:40:26,188 - INFO - Loading 1 samples from The Pile (train split)...
2025-10-10 13:40:27,918 - INFO - Loaded 1 samples from The Pile
2025-10-10 13:40:27,918 - INFO - ================================================================================
2025-10-10 13:40:27,918 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:40:27,918 - INFO - ================================================================================
2025-10-10 13:40:27,918 - INFO - 
================================================================================
2025-10-10 13:40:27,918 - INFO - Evaluating: tinystories-33M
2025-10-10 13:40:27,918 - INFO - ================================================================================
2025-10-10 13:40:27,918 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:40:29,814 - WARNING - Baseline         No GPUs available. Using cpu.
2025-10-10 13:40:31,904 - INFO - tinystories-33M                 Baseline         PPL 8.59  Xent 2.1503  Entr 5.8763
2025-10-10 13:40:32,104 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-10 13:40:32,104 - INFO - --- MX (Selective GEMM) [TierA] --- Skipped (--skip_mx)
2025-10-10 13:40:32,125 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [A]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:40:32,125 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-10 13:40:32,126 - INFO - --- MX (Selective GEMM) [TierB] --- Skipped (--skip_mx)
2025-10-10 13:40:32,126 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [B]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:40:32,126 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-10 13:40:32,126 - INFO - --- MX (Selective GEMM) [TierC] --- Skipped (--skip_mx)
2025-10-10 13:40:32,126 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [C]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:40:32,126 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-10 13:40:32,126 - INFO - --- MX (Selective GEMM) [TierD] --- Skipped (--skip_mx)
2025-10-10 13:40:32,127 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [D]: No eligible CUDA devices for NV-FP8 backend
2025-10-10 13:40:32,127 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-10 13:40:32,127 - WARNING - Skipping INT8 benchmark for tinystories-33M: No eligible CUDA devices for INT8 backend
2025-10-10 13:40:32,127 - INFO - 
================================================================================
2025-10-10 13:40:32,127 - INFO - BENCHMARK COMPLETE
2025-10-10 13:40:32,127 - INFO - ================================================================================
2025-10-10 13:42:37,463 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:42:37,463 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:42:37,463 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:42:37,463 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:42:37,463 - INFO - Loading evaluation data...
2025-10-10 13:42:37,463 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-10 13:42:39,199 - INFO - Loaded 1000 samples from The Pile
2025-10-10 13:42:39,199 - INFO - ================================================================================
2025-10-10 13:42:39,199 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:42:39,200 - INFO - ================================================================================
2025-10-10 13:42:39,200 - INFO - 
================================================================================
2025-10-10 13:42:39,200 - INFO - Evaluating: tinystories-33M
2025-10-10 13:42:39,200 - INFO - ================================================================================
2025-10-10 13:42:39,200 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:42:40,970 - WARNING - Baseline         No GPUs available. Using cpu.
2025-10-10 13:43:32,753 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:43:32,754 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:43:32,754 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:43:32,754 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:43:32,754 - INFO - Loading evaluation data...
2025-10-10 13:43:32,754 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-10 13:43:34,893 - INFO - Loaded 1000 samples from The Pile
2025-10-10 13:43:34,893 - INFO - ================================================================================
2025-10-10 13:43:34,893 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:43:34,893 - INFO - ================================================================================
2025-10-10 13:43:34,893 - INFO - 
================================================================================
2025-10-10 13:43:34,893 - INFO - Evaluating: tinystories-33M
2025-10-10 13:43:34,893 - INFO - ================================================================================
2025-10-10 13:43:34,894 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:43:37,080 - WARNING - Baseline         No GPUs available. Using cpu.
2025-10-10 13:44:55,408 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-10 13:44:55,408 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-10 13:44:55,408 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-10 13:44:55,408 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-10 13:44:55,408 - INFO - Loading evaluation data...
2025-10-10 13:44:55,408 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-10 13:44:57,197 - INFO - Loaded 1000 samples from The Pile
2025-10-10 13:44:57,197 - INFO - ================================================================================
2025-10-10 13:44:57,197 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-10 13:44:57,197 - INFO - ================================================================================
2025-10-10 13:44:57,197 - INFO - 
================================================================================
2025-10-10 13:44:57,197 - INFO - Evaluating: tinystories-33M
2025-10-10 13:44:57,197 - INFO - ================================================================================
2025-10-10 13:44:57,197 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:44:58,895 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:45:13,028 - INFO - tinystories-33M                 Baseline         PPL 5.03  Xent 1.6149  Entr 4.5666
2025-10-10 13:45:33,561 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-10 13:45:33,591 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:45:42,377 - INFO - tinystories-33M-BF16            BF16             PPL 5.05  Xent 1.6202  Entr 4.5811
2025-10-10 13:46:02,858 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-10 13:46:02,858 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:46:02,860 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:46:04,310 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:46:04,311 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:46:16,483 - INFO - tinystories-33M-BFP16-TierA     BFP16-TierA      PPL 5.03  Xent 1.6153  Entr 4.5674
2025-10-10 13:46:37,052 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:46:37,053 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:46:37,054 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:46:38,542 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:46:38,543 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:46:40,031 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:46:40,031 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:46:58,663 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 5.21  Xent 1.6509  Entr 4.6592
2025-10-10 13:47:00,210 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 5.03  Xent 1.6157  Entr 4.5695
2025-10-10 13:47:19,240 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:47:19,242 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:47:20,640 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:47:20,641 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:47:53,742 - INFO - tinystories-33M-TierA           MX-TierA         PPL 5.15  Xent 1.6381  Entr 4.6228
2025-10-10 13:48:14,269 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:48:15,741 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-10 13:48:17,631 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 13:48:17,632 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-10 13:48:17,633 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:48:19,020 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:48:19,021 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:48:31,641 - INFO - tinystories-33M-BFP16-TierB     BFP16-TierB      PPL 5.03  Xent 1.6152  Entr 4.5674
2025-10-10 13:48:52,218 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:48:53,519 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:48:53,521 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:48:55,618 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:48:55,620 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:49:20,242 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 4.98  Xent 1.6049  Entr 4.5459
2025-10-10 13:49:22,432 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 4.66  Xent 1.5401  Entr 4.3662
2025-10-10 13:49:40,877 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:49:42,341 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:49:42,343 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:49:55,117 - INFO - tinystories-33M-TierB           MX-TierB         PPL 5.15  Xent 1.6384  Entr 4.6212
2025-10-10 13:50:35,730 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:50:37,324 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-10 13:50:39,266 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 13:50:39,267 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-10 13:50:39,267 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:50:40,652 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:50:40,653 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:50:52,901 - INFO - tinystories-33M-BFP16-TierC     BFP16-TierC      PPL 5.03  Xent 1.6152  Entr 4.5674
2025-10-10 13:51:13,506 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:51:14,814 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:51:14,815 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:51:16,815 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:51:16,816 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:51:41,270 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 4.98  Xent 1.6049  Entr 4.5459
2025-10-10 13:51:42,906 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 4.66  Xent 1.5401  Entr 4.3662
2025-10-10 13:52:01,925 - INFO - MX Configuration (fp6_e3m2): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e3m2",
    "a_elem_format": "fp6_e3m2",
    "w_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp_ex": "fp6_e3m2",
    "a_elem_format_bp_os": "fp6_e3m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:52:01,927 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:52:03,313 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:52:03,314 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:52:03,315 - INFO - Replaced 16 Linear layers with MxLinear
2025-10-10 13:52:16,212 - INFO - tinystories-33M-TierC           MX-TierC         PPL 5.04  Xent 1.6168  Entr 4.5687
2025-10-10 13:52:56,836 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:52:58,264 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-10 13:53:00,323 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 13:53:00,323 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-10 13:53:00,324 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:53:01,677 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:53:01,678 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:53:14,159 - INFO - tinystories-33M-BFP16-TierD     BFP16-TierD      PPL 5.03  Xent 1.6152  Entr 4.5674
2025-10-10 13:53:34,754 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:53:36,191 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:53:36,192 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:53:37,571 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-10 13:53:37,572 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-10 13:54:04,260 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 4.98  Xent 1.6049  Entr 4.5459
2025-10-10 13:54:05,602 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 4.66  Xent 1.5401  Entr 4.3662
2025-10-10 13:54:24,949 - INFO - MX Configuration (fp4_e2m1): {
    "scale_bits": 8,
    "w_elem_format": "fp4_e2m1",
    "a_elem_format": "fp4_e2m1",
    "w_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp_ex": "fp4_e2m1",
    "a_elem_format_bp_os": "fp4_e2m1",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-10 13:54:24,954 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:54:26,405 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-10 13:54:26,406 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-10 13:54:26,407 - INFO - Replaced 16 Linear layers with MxLinear
2025-10-10 13:54:39,358 - INFO - tinystories-33M-TierD           MX-TierD         PPL 5.30  Xent 1.6671  Entr 4.7085
2025-10-10 13:55:20,003 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:55:21,552 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-10 13:55:23,551 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 13:55:23,552 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-10 13:55:23,553 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:55:39,366 - INFO - tinystories-33M-INT8            INT8             PPL 5.09  Xent 1.6275  Entr nan
2025-10-10 13:55:59,729 - INFO - 
================================================================================
2025-10-10 13:55:59,730 - INFO - Evaluating: phi-1_5
2025-10-10 13:55:59,730 - INFO - ================================================================================
2025-10-10 13:55:59,730 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 13:55:59,733 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:58:13,398 - INFO - phi-1_5                         Baseline         PPL 2.35  Xent 0.8563  Entr 2.4214
2025-10-10 13:58:38,198 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-10 13:58:38,200 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:59:19,004 - INFO - phi-1_5-BF16                    BF16             PPL 2.35  Xent 0.8564  Entr 2.4185
2025-10-10 13:59:41,787 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-10 13:59:41,787 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 13:59:41,788 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 13:59:43,347 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 13:59:43,349 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-10 14:02:00,174 - INFO - phi-1_5-BFP16-TierA             BFP16-TierA      PPL 2.35  Xent 0.8562  Entr 2.4208
2025-10-10 14:02:25,081 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 14:02:25,081 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 14:02:25,082 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:02:26,847 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:02:26,850 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-10 14:02:28,846 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:02:28,848 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-10 14:07:34,325 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 2.36  Xent 0.8584  Entr 2.4458
2025-10-10 14:07:35,285 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 2.36  Xent 0.8580  Entr 2.4414
2025-10-10 14:08:20,396 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:08:22,110 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-10 14:08:22,112 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-10 14:10:42,200 - INFO - phi-1_5-TierA                   MX-TierA         PPL 2.40  Xent 0.8763  Entr 2.5033
2025-10-10 14:11:07,293 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:11:10,842 - INFO - Replaced 48 Linear layers with NvFp8Linear
2025-10-10 14:11:15,840 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 14:11:15,840 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-10 14:11:15,846 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:11:17,718 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:11:17,722 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:13:36,026 - INFO - phi-1_5-BFP16-TierB             BFP16-TierB      PPL 2.35  Xent 0.8563  Entr 2.4210
2025-10-10 14:14:01,030 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:14:02,720 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:14:02,723 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:14:04,675 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:14:04,679 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:21:00,498 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 2.39  Xent 0.8730  Entr 2.4991
2025-10-10 14:21:03,379 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 2.40  Xent 0.8769  Entr 2.5120
2025-10-10 14:21:46,922 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:21:48,669 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-10 14:21:48,673 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:24:10,750 - INFO - phi-1_5-TierB                   MX-TierB         PPL 2.41  Xent 0.8800  Entr 2.5209
2025-10-10 14:24:35,807 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:24:39,171 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-10 14:24:44,197 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 14:24:44,198 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-10 14:24:44,199 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:24:45,740 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:24:45,744 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:27:04,743 - INFO - phi-1_5-BFP16-TierC             BFP16-TierC      PPL 2.35  Xent 0.8563  Entr 2.4210
2025-10-10 14:27:29,685 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:27:31,270 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:27:31,275 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:27:33,078 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:27:33,082 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:34:28,774 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 2.39  Xent 0.8730  Entr 2.4991
2025-10-10 14:34:31,711 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 2.40  Xent 0.8769  Entr 2.5120
2025-10-10 14:35:14,046 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:35:15,911 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-10 14:35:15,913 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-10 14:35:15,915 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-10 14:37:37,325 - INFO - phi-1_5-TierC                   MX-TierC         PPL 2.40  Xent 0.8775  Entr 2.5133
2025-10-10 14:38:01,955 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:38:05,036 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-10 14:38:10,764 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 14:38:10,764 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-10 14:38:10,765 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:38:12,418 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:38:12,422 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:40:31,322 - INFO - phi-1_5-BFP16-TierD             BFP16-TierD      PPL 2.35  Xent 0.8563  Entr 2.4210
2025-10-10 14:40:56,063 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:40:57,704 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:40:57,709 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:40:59,642 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-10 14:40:59,646 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-10 14:47:54,710 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 2.39  Xent 0.8730  Entr 2.4991
2025-10-10 14:47:57,528 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 2.40  Xent 0.8769  Entr 2.5120
2025-10-10 14:48:39,939 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:48:42,847 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-10 14:48:42,849 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-10 14:48:42,851 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-10 14:51:02,873 - INFO - phi-1_5-TierD                   MX-TierD         PPL 2.49  Xent 0.9122  Entr 2.7211
2025-10-10 14:51:27,615 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:51:30,671 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-10 14:51:36,332 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 14:51:36,332 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-10 14:51:36,333 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:52:50,610 - INFO - phi-1_5-INT8                    INT8             PPL 2.35  Xent 0.8554  Entr nan
2025-10-10 14:53:11,209 - INFO - 
================================================================================
2025-10-10 14:53:11,210 - INFO - Evaluating: llama-3.1-8b
2025-10-10 14:53:11,210 - INFO - ================================================================================
2025-10-10 14:53:11,210 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-10 14:53:11,211 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:55:33,363 - INFO - llama-3.1-8b                    Baseline         PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-10 14:56:05,721 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-10 14:56:05,725 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:58:15,851 - INFO - llama-3.1-8b-BF16               BF16             PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-10 14:58:47,994 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-10 14:58:47,995 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 14:58:47,996 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 14:58:49,662 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-10 14:58:49,665 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-10 15:01:00,436 - INFO - llama-3.1-8b-BFP16-TierA        BFP16-TierA      PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-10 15:01:32,771 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 15:01:32,772 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-10 15:01:32,773 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 15:01:34,397 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-10 15:01:34,401 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-10 15:01:36,184 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-10 15:01:36,187 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-10 15:13:27,910 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 1.75  Xent 0.5569  Entr 1.5915
2025-10-10 15:13:32,168 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 1.83  Xent 0.6061  Entr 1.7752
2025-10-10 15:14:22,400 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 15:14:24,023 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-10 15:14:24,026 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-10 15:17:15,832 - INFO - llama-3.1-8b-TierA              MX-TierA         PPL 1.76  Xent 0.5666  Entr 1.6125
2025-10-10 15:17:48,085 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 15:17:52,767 - INFO - Replaced 96 Linear layers with NvFp8Linear
2025-10-10 15:18:05,312 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-10 15:18:05,313 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-10 15:18:05,314 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 15:18:06,887 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-10 15:18:06,893 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-10 15:20:15,987 - INFO - llama-3.1-8b-BFP16-TierB        BFP16-TierB      PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-10 15:20:48,438 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-10 15:20:50,020 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-10 15:20:50,026 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-10 15:20:51,851 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-10 15:20:51,857 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 15:51:07,173 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-12 15:51:07,176 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-12 15:51:07,176 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-12 15:51:07,176 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-12 15:51:07,176 - INFO - Loading evaluation data...
2025-10-12 15:51:07,176 - INFO - Loading 1000 samples from The Pile (train split)...
2025-10-12 15:51:32,645 - INFO - Loaded 1000 samples from The Pile
2025-10-12 15:51:32,645 - INFO - ================================================================================
2025-10-12 15:51:32,645 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-12 15:51:32,645 - INFO - ================================================================================
2025-10-12 15:51:32,645 - INFO - 
================================================================================
2025-10-12 15:51:32,645 - INFO - Evaluating: tinystories-33M
2025-10-12 15:51:32,645 - INFO - ================================================================================
2025-10-12 15:51:32,645 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-12 15:51:35,166 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:52:06,977 - INFO - tinystories-33M                 Baseline         PPL 5.03  Xent 1.6149  Entr 4.5666
2025-10-12 15:52:27,575 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-12 15:52:27,604 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:52:43,137 - INFO - tinystories-33M-BF16            BF16             PPL 5.05  Xent 1.6202  Entr 4.5811
2025-10-12 15:53:03,602 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-12 15:53:03,603 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 15:53:03,605 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:53:04,989 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:53:04,990 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-12 15:53:28,943 - INFO - tinystories-33M-BFP16-TierA     BFP16-TierA      PPL 5.03  Xent 1.6153  Entr 4.5674
2025-10-12 15:53:49,534 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 15:53:49,535 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 15:53:49,536 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:53:50,885 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:53:50,885 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-12 15:53:52,228 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:53:52,234 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-12 15:54:31,430 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 5.21  Xent 1.6509  Entr 4.6592
2025-10-12 15:54:33,414 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 5.03  Xent 1.6157  Entr 4.5695
2025-10-12 15:54:52,034 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-12 15:54:52,035 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:54:53,425 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-12 15:54:53,426 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-12 15:55:39,675 - INFO - tinystories-33M-TierA           MX-TierA         PPL 5.15  Xent 1.6381  Entr 4.6228
2025-10-12 15:56:00,226 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:56:01,736 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-12 15:56:04,070 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 15:56:04,071 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-12 15:56:04,088 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:56:05,545 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:56:05,546 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 15:56:30,820 - INFO - tinystories-33M-BFP16-TierB     BFP16-TierB      PPL 5.03  Xent 1.6152  Entr 4.5674
2025-10-12 15:56:51,459 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:56:52,839 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:56:52,840 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 15:56:54,268 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:56:54,274 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 15:57:43,572 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 4.98  Xent 1.6049  Entr 4.5459
2025-10-12 15:57:45,608 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 4.66  Xent 1.5401  Entr 4.3662
2025-10-12 15:58:04,333 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:58:06,005 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-12 15:58:06,006 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 15:58:32,531 - INFO - tinystories-33M-TierB           MX-TierB         PPL 5.15  Xent 1.6384  Entr 4.6212
2025-10-12 15:59:13,192 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:59:14,758 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-12 15:59:16,910 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 15:59:16,910 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-12 15:59:16,911 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 15:59:18,323 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 15:59:18,325 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 15:59:44,131 - INFO - tinystories-33M-BFP16-TierC     BFP16-TierC      PPL 5.03  Xent 1.6152  Entr 4.5674
2025-10-12 16:00:04,750 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:00:06,321 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 16:00:06,322 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 16:00:08,601 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 16:00:08,603 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 16:00:56,913 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 4.98  Xent 1.6049  Entr 4.5459
2025-10-12 16:00:58,940 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 4.66  Xent 1.5401  Entr 4.3662
2025-10-12 16:01:17,563 - INFO - MX Configuration (fp6_e3m2): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e3m2",
    "a_elem_format": "fp6_e3m2",
    "w_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp_ex": "fp6_e3m2",
    "a_elem_format_bp_os": "fp6_e3m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-12 16:01:17,564 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:01:18,922 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-12 16:01:18,922 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-12 16:01:18,923 - INFO - Replaced 16 Linear layers with MxLinear
2025-10-12 16:01:45,207 - INFO - tinystories-33M-TierC           MX-TierC         PPL 5.04  Xent 1.6168  Entr 4.5687
2025-10-12 16:02:25,889 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:02:27,506 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-12 16:02:29,440 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 16:02:29,440 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-12 16:02:29,441 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:02:30,803 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 16:02:30,804 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 16:02:56,691 - INFO - tinystories-33M-BFP16-TierD     BFP16-TierD      PPL 5.03  Xent 1.6152  Entr 4.5674
2025-10-12 16:03:17,323 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:03:18,782 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 16:03:18,783 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 16:03:20,195 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-12 16:03:20,202 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-12 16:04:09,315 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 4.98  Xent 1.6049  Entr 4.5459
2025-10-12 16:04:11,476 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 4.66  Xent 1.5401  Entr 4.3662
2025-10-12 16:04:30,024 - INFO - MX Configuration (fp4_e2m1): {
    "scale_bits": 8,
    "w_elem_format": "fp4_e2m1",
    "a_elem_format": "fp4_e2m1",
    "w_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp_ex": "fp4_e2m1",
    "a_elem_format_bp_os": "fp4_e2m1",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-12 16:04:30,026 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:04:31,426 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-12 16:04:31,427 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-12 16:04:31,428 - INFO - Replaced 16 Linear layers with MxLinear
2025-10-12 16:04:57,953 - INFO - tinystories-33M-TierD           MX-TierD         PPL 5.30  Xent 1.6671  Entr 4.7085
2025-10-12 16:05:38,603 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:05:40,254 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-12 16:05:42,195 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 16:05:42,195 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-12 16:05:42,196 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:06:25,134 - INFO - tinystories-33M-INT8            INT8             PPL 5.09  Xent 1.6275  Entr nan
2025-10-12 16:06:45,510 - INFO - 
================================================================================
2025-10-12 16:06:45,511 - INFO - Evaluating: phi-1_5
2025-10-12 16:06:45,511 - INFO - ================================================================================
2025-10-12 16:06:45,511 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-12 16:06:45,513 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:11:10,064 - INFO - phi-1_5                         Baseline         PPL 2.35  Xent 0.8563  Entr 2.4214
2025-10-12 16:11:35,324 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-12 16:11:35,325 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:12:56,362 - INFO - phi-1_5-BF16                    BF16             PPL 2.35  Xent 0.8564  Entr 2.4185
2025-10-12 16:13:19,355 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-12 16:13:19,356 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 16:13:19,366 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:13:21,208 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 16:13:21,210 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-12 16:17:50,349 - INFO - phi-1_5-BFP16-TierA             BFP16-TierA      PPL 2.35  Xent 0.8562  Entr 2.4208
2025-10-12 16:18:15,414 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 16:18:15,414 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 16:18:15,415 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:18:17,361 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 16:18:17,363 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-12 16:18:19,296 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 16:18:19,298 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-12 16:28:52,770 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 2.36  Xent 0.8584  Entr 2.4458
2025-10-12 16:29:04,786 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 2.36  Xent 0.8580  Entr 2.4414
2025-10-12 16:29:18,987 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:29:22,143 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-12 16:29:22,145 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-12 16:33:58,244 - INFO - phi-1_5-TierA                   MX-TierA         PPL 2.40  Xent 0.8763  Entr 2.5033
2025-10-12 16:34:43,682 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:34:47,503 - INFO - Replaced 48 Linear layers with NvFp8Linear
2025-10-12 16:34:52,758 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 16:34:52,758 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-12 16:34:52,759 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:34:54,713 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 16:34:54,717 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 16:39:31,167 - INFO - phi-1_5-BFP16-TierB             BFP16-TierB      PPL 2.35  Xent 0.8563  Entr 2.4210
2025-10-12 16:39:56,547 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:39:58,567 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 16:39:58,571 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 16:40:00,552 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 16:40:00,556 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 16:54:33,360 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 2.39  Xent 0.8730  Entr 2.4991
2025-10-12 16:54:43,316 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 2.40  Xent 0.8769  Entr 2.5120
2025-10-12 16:54:59,563 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 16:55:01,568 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-12 16:55:01,573 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 16:59:49,362 - INFO - phi-1_5-TierB                   MX-TierB         PPL 2.41  Xent 0.8800  Entr 2.5209
2025-10-12 17:00:34,451 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:00:38,345 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-12 17:00:44,105 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 17:00:44,105 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-12 17:00:44,106 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:00:46,033 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 17:00:46,037 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 17:05:23,673 - INFO - phi-1_5-BFP16-TierC             BFP16-TierC      PPL 2.35  Xent 0.8563  Entr 2.4210
2025-10-12 17:05:48,651 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:05:50,648 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 17:05:50,653 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 17:05:52,790 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 17:05:52,794 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 17:20:26,079 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 2.39  Xent 0.8730  Entr 2.4991
2025-10-12 17:20:35,566 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 2.40  Xent 0.8769  Entr 2.5120
2025-10-12 17:20:51,007 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:20:53,061 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-12 17:20:53,063 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-12 17:20:53,065 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 17:25:39,171 - INFO - phi-1_5-TierC                   MX-TierC         PPL 2.40  Xent 0.8775  Entr 2.5133
2025-10-12 17:26:24,539 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:26:28,401 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-12 17:26:33,680 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 17:26:33,681 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-12 17:26:33,682 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:26:35,651 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 17:26:35,655 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 17:31:12,887 - INFO - phi-1_5-BFP16-TierD             BFP16-TierD      PPL 2.35  Xent 0.8563  Entr 2.4210
2025-10-12 17:31:38,131 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:31:40,099 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 17:31:40,104 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 17:31:41,945 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-12 17:31:41,950 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-12 17:39:44,791 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 2.40  Xent 0.8769  Entr 2.5120
2025-10-12 17:40:02,803 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 2.39  Xent 0.8730  Entr 2.4991
2025-10-12 17:40:09,996 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:40:11,627 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-12 17:40:11,628 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-12 17:40:11,631 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 17:42:33,454 - INFO - phi-1_5-TierD                   MX-TierD         PPL 2.49  Xent 0.9122  Entr 2.7211
2025-10-12 17:43:18,781 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:43:22,171 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-12 17:43:27,042 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 17:43:27,042 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-12 17:43:27,043 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:44:40,614 - INFO - phi-1_5-INT8                    INT8             PPL 2.35  Xent 0.8554  Entr nan
2025-10-12 17:45:01,284 - INFO - 
================================================================================
2025-10-12 17:45:01,284 - INFO - Evaluating: llama-3.1-8b
2025-10-12 17:45:01,284 - INFO - ================================================================================
2025-10-12 17:45:01,284 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-12 17:45:01,285 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:47:15,425 - INFO - llama-3.1-8b                    Baseline         PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-12 17:47:47,570 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-12 17:47:47,572 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:49:57,035 - INFO - llama-3.1-8b-BF16               BF16             PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-12 17:50:29,955 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-12 17:50:29,956 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 17:50:29,960 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:50:31,491 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 17:50:31,494 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 17:52:40,515 - INFO - llama-3.1-8b-BFP16-TierA        BFP16-TierA      PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-12 17:53:13,246 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 17:53:13,247 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-12 17:53:13,248 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 17:53:15,128 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 17:53:15,131 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 17:53:17,037 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 17:53:17,040 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 18:05:07,200 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 1.75  Xent 0.5569  Entr 1.5915
2025-10-12 18:05:10,953 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 1.83  Xent 0.6061  Entr 1.7752
2025-10-12 18:06:01,778 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:06:03,791 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:06:03,793 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 18:08:55,376 - INFO - llama-3.1-8b-TierA              MX-TierA         PPL 1.76  Xent 0.5666  Entr 1.6125
2025-10-12 18:09:27,873 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:09:33,944 - INFO - Replaced 96 Linear layers with NvFp8Linear
2025-10-12 18:09:46,723 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 18:09:46,724 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-12 18:09:46,725 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:09:48,279 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:09:48,284 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:11:58,780 - INFO - llama-3.1-8b-BFP16-TierB        BFP16-TierB      PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-12 18:12:32,281 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:12:33,827 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:12:33,832 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:12:35,687 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:12:35,692 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:27:51,433 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 1.70  Xent 0.5313  Entr 1.4505
2025-10-12 18:27:55,166 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 1.75  Xent 0.5573  Entr 1.5657
2025-10-12 18:28:46,916 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:28:48,727 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:28:48,733 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:31:55,622 - INFO - llama-3.1-8b-TierB              MX-TierB         PPL 1.73  Xent 0.5460  Entr 1.5141
2025-10-12 18:32:27,836 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:32:33,761 - INFO - Replaced 224 Linear layers with NvFp8Linear
2025-10-12 18:32:46,172 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 18:32:46,172 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-12 18:32:46,178 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:32:47,804 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:32:47,809 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:34:57,208 - INFO - llama-3.1-8b-BFP16-TierC        BFP16-TierC      PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-12 18:35:30,336 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:35:31,924 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:35:31,929 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:35:33,752 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:35:33,757 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:50:48,989 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 1.70  Xent 0.5313  Entr 1.4505
2025-10-12 18:50:53,134 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 1.75  Xent 0.5573  Entr 1.5657
2025-10-12 18:51:44,184 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:51:45,833 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:51:45,836 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 18:51:45,839 - INFO - Replaced 128 Linear layers with MxLinear
2025-10-12 18:54:51,874 - INFO - llama-3.1-8b-TierC              MX-TierC         PPL 1.77  Xent 0.5727  Entr 1.6365
2025-10-12 18:55:24,784 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:55:30,474 - INFO - Replaced 224 Linear layers with NvFp8Linear
2025-10-12 18:55:42,386 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 18:55:42,386 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-12 18:55:42,387 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:55:43,968 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:55:43,973 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:57:54,038 - INFO - llama-3.1-8b-BFP16-TierD        BFP16-TierD      PPL 1.74  Xent 0.5548  Entr 1.5398
2025-10-12 18:58:27,112 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 18:58:28,755 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:58:28,761 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 18:58:30,685 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-12 18:58:30,690 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-12 19:13:45,209 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 1.70  Xent 0.5313  Entr 1.4505
2025-10-12 19:13:49,827 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 1.75  Xent 0.5573  Entr 1.5657
2025-10-12 19:14:40,153 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 19:14:41,863 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-12 19:14:41,865 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-12 19:14:41,868 - INFO - Replaced 128 Linear layers with MxLinear
2025-10-12 19:17:46,338 - INFO - llama-3.1-8b-TierD              MX-TierD         PPL 2.27  Xent 0.8209  Entr 2.4065
2025-10-12 19:18:18,645 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 19:18:24,359 - INFO - Replaced 224 Linear layers with NvFp8Linear
2025-10-12 19:18:36,567 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-12 19:18:36,568 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-12 19:18:36,568 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-12 19:21:23,386 - INFO - llama-3.1-8b-INT8               INT8             PPL 1.80  Xent 0.5869  Entr nan
2025-10-12 19:21:44,648 - INFO - 
================================================================================
2025-10-12 19:21:44,649 - INFO - BENCHMARK COMPLETE
2025-10-12 19:21:44,649 - INFO - ================================================================================
2025-10-14 11:55:01,706 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-10-14 11:55:01,706 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-10-14 11:55:01,706 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-10-14 11:55:01,706 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-10-14 11:55:01,707 - INFO - Loading evaluation data...
2025-10-14 11:55:01,707 - INFO - Loading 20000 samples from The Pile (train split)...
2025-10-14 11:55:06,199 - INFO - Loaded 20000 samples from The Pile
2025-10-14 11:55:06,200 - INFO - ================================================================================
2025-10-14 11:55:06,200 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-14 11:55:06,200 - INFO - ================================================================================
2025-10-14 11:55:06,200 - INFO - 
================================================================================
2025-10-14 11:55:06,200 - INFO - Evaluating: tinystories-33M
2025-10-14 11:55:06,200 - INFO - ================================================================================
2025-10-14 11:55:06,200 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-14 11:55:07,916 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 11:58:59,821 - INFO - tinystories-33M                 Baseline         PPL 5.11  Xent 1.6320  Entr 4.6064
2025-10-14 11:59:20,452 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-14 11:59:20,486 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:01:32,286 - INFO - tinystories-33M-BF16            BF16             PPL 5.14  Xent 1.6375  Entr 4.6200
2025-10-14 12:01:52,778 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-14 12:01:52,779 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-14 12:01:52,783 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:01:54,191 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:01:54,192 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-14 12:05:43,409 - INFO - tinystories-33M-BFP16-TierA     BFP16-TierA      PPL 5.12  Xent 1.6324  Entr 4.6072
2025-10-14 12:06:04,087 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-14 12:06:04,088 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-14 12:06:04,092 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:06:05,575 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:06:05,576 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-14 12:06:07,701 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:06:07,702 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-14 12:12:40,946 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 5.30  Xent 1.6678  Entr 4.6952
2025-10-14 12:12:42,349 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 5.13  Xent 1.6355  Entr 4.6126
2025-10-14 12:13:01,639 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-14 12:13:01,640 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:13:03,248 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-14 12:13:03,249 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-14 12:17:15,582 - INFO - tinystories-33M-TierA           MX-TierA         PPL 5.22  Xent 1.6533  Entr 4.6588
2025-10-14 12:17:36,124 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:17:37,711 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-10-14 12:17:59,746 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-14 12:17:59,746 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-14 12:17:59,748 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:18:01,134 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:18:01,135 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:21:53,424 - INFO - tinystories-33M-BFP16-TierB     BFP16-TierB      PPL 5.11  Xent 1.6321  Entr 4.6067
2025-10-14 12:22:14,101 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:22:15,598 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:22:15,599 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:22:17,404 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:22:17,405 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:31:05,069 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 5.06  Xent 1.6217  Entr 4.5788
2025-10-14 12:31:09,002 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 4.73  Xent 1.5547  Entr 4.3951
2025-10-14 12:31:25,810 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:31:27,424 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-14 12:31:27,425 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:35:44,566 - INFO - tinystories-33M-TierB           MX-TierB         PPL 5.23  Xent 1.6539  Entr 4.6549
2025-10-14 12:36:05,165 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:36:07,211 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-14 12:36:24,899 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-14 12:36:24,899 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-14 12:36:24,900 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:36:26,317 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:36:26,318 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:40:22,212 - INFO - tinystories-33M-BFP16-TierC     BFP16-TierC      PPL 5.11  Xent 1.6321  Entr 4.6067
2025-10-14 12:40:42,899 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:40:44,440 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:40:44,441 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:40:46,552 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:40:46,553 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:49:26,034 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 5.06  Xent 1.6217  Entr 4.5788
2025-10-14 12:49:28,923 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 4.73  Xent 1.5547  Entr 4.3951
2025-10-14 12:49:46,838 - INFO - MX Configuration (fp6_e3m2): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e3m2",
    "a_elem_format": "fp6_e3m2",
    "w_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp_ex": "fp6_e3m2",
    "a_elem_format_bp_os": "fp6_e3m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-14 12:49:46,842 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:49:48,363 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-14 12:49:48,364 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-14 12:49:48,364 - INFO - Replaced 16 Linear layers with MxLinear
2025-10-14 12:54:09,597 - INFO - tinystories-33M-TierC           MX-TierC         PPL 5.13  Xent 1.6347  Entr 4.6083
2025-10-14 12:54:30,265 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:54:32,032 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-14 12:54:51,522 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-14 12:54:51,522 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-14 12:54:51,525 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:54:52,955 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:54:52,956 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:58:49,919 - INFO - tinystories-33M-BFP16-TierD     BFP16-TierD      PPL 5.11  Xent 1.6321  Entr 4.6067
2025-10-14 12:59:10,660 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 12:59:12,140 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:59:12,141 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 12:59:13,883 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-10-14 12:59:13,884 - INFO - Replaced 24 Linear layers with MxLinear
2025-10-14 13:07:46,190 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 5.06  Xent 1.6217  Entr 4.5788
2025-10-14 13:07:46,309 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 4.73  Xent 1.5547  Entr 4.3951
2025-10-14 13:08:27,008 - INFO - MX Configuration (fp4_e2m1): {
    "scale_bits": 8,
    "w_elem_format": "fp4_e2m1",
    "a_elem_format": "fp4_e2m1",
    "w_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp_ex": "fp4_e2m1",
    "a_elem_format_bp_os": "fp4_e2m1",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-10-14 13:08:27,010 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 13:08:30,194 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-10-14 13:08:30,195 - INFO - Replaced 8 Linear layers with MxLinear
2025-10-14 13:08:30,196 - INFO - Replaced 16 Linear layers with MxLinear
2025-10-14 13:12:29,143 - INFO - tinystories-33M-TierD           MX-TierD         PPL 5.35  Xent 1.6767  Entr 4.7260
2025-10-14 13:12:49,755 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 13:12:51,304 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-10-14 13:13:13,153 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-14 13:13:13,154 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-14 13:13:13,155 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 13:16:56,110 - INFO - tinystories-33M-INT8            INT8             PPL 5.18  Xent 1.6446  Entr nan
2025-10-14 13:17:16,525 - INFO - 
================================================================================
2025-10-14 13:17:16,526 - INFO - Evaluating: phi-1_5
2025-10-14 13:17:16,526 - INFO - ================================================================================
2025-10-14 13:17:16,526 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-14 13:17:16,534 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 14:01:34,686 - INFO - phi-1_5                         Baseline         PPL 2.35  Xent 0.8554  Entr 2.4281
2025-10-14 14:01:59,382 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-14 14:01:59,384 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 14:14:45,917 - INFO - phi-1_5-BF16                    BF16             PPL 2.35  Xent 0.8553  Entr 2.4248
2025-10-14 14:15:09,139 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-14 14:15:09,140 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-14 14:15:09,141 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 14:15:10,815 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 14:15:10,817 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-14 15:00:45,142 - INFO - phi-1_5-BFP16-TierA             BFP16-TierA      PPL 2.35  Xent 0.8553  Entr 2.4275
2025-10-14 15:01:09,893 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-14 15:01:09,893 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-14 15:01:09,895 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 15:01:12,774 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 15:01:12,776 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-14 15:01:15,053 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 15:01:15,061 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-14 16:42:41,910 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 2.36  Xent 0.8573  Entr 2.4502
2025-10-14 16:43:22,085 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 2.36  Xent 0.8572  Entr 2.4529
2025-10-14 16:43:46,849 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 16:43:48,547 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-14 16:43:48,549 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-14 17:29:55,868 - INFO - phi-1_5-TierA                   MX-TierA         PPL 2.40  Xent 0.8755  Entr 2.5115
2025-10-14 17:30:20,745 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 17:30:24,489 - INFO - Replaced 48 Linear layers with NvFp8Linear
2025-10-14 17:30:51,977 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-14 17:30:51,978 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-14 17:30:51,979 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 17:30:53,531 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 17:30:53,535 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-14 18:16:58,790 - INFO - phi-1_5-BFP16-TierB             BFP16-TierB      PPL 2.35  Xent 0.8553  Entr 2.4278
2025-10-14 18:17:23,600 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 18:17:25,340 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 18:17:25,343 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-14 18:17:27,198 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 18:17:27,202 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-14 20:36:07,423 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 2.39  Xent 0.8721  Entr 2.5052
2025-10-14 20:36:50,735 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 2.40  Xent 0.8767  Entr 2.5203
2025-10-14 20:37:15,619 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 20:37:17,263 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-14 20:37:17,267 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-14 21:24:27,543 - INFO - phi-1_5-TierB                   MX-TierB         PPL 2.41  Xent 0.8787  Entr 2.5275
2025-10-14 21:24:52,247 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 21:24:55,570 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-14 21:25:18,901 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-14 21:25:18,902 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-14 21:25:18,902 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 21:25:20,493 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 21:25:20,496 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-14 22:11:10,382 - INFO - phi-1_5-BFP16-TierC             BFP16-TierC      PPL 2.35  Xent 0.8553  Entr 2.4278
2025-10-14 22:11:35,301 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-14 22:11:37,188 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 22:11:37,193 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-14 22:11:39,108 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-14 22:11:39,112 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-15 00:30:20,498 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 2.39  Xent 0.8721  Entr 2.5052
2025-10-15 00:31:02,785 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 2.40  Xent 0.8767  Entr 2.5203
2025-10-15 00:31:27,329 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 00:31:29,045 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-15 00:31:29,047 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-15 00:31:29,049 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-15 01:18:08,666 - INFO - phi-1_5-TierC                   MX-TierC         PPL 2.40  Xent 0.8769  Entr 2.5214
2025-10-15 01:18:33,347 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 01:18:37,026 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-15 01:18:58,565 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-15 01:18:58,566 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-15 01:18:58,567 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 01:19:00,219 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-15 01:19:00,222 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-15 02:04:45,108 - INFO - phi-1_5-BFP16-TierD             BFP16-TierD      PPL 2.35  Xent 0.8553  Entr 2.4278
2025-10-15 02:05:09,930 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 02:05:11,875 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-15 02:05:11,879 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-15 02:05:13,983 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-15 02:05:13,991 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-15 04:23:50,806 - INFO - phi-1_5-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 2.39  Xent 0.8721  Entr 2.5052
2025-10-15 04:24:37,585 - INFO - phi-1_5-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 2.40  Xent 0.8767  Entr 2.5203
2025-10-15 04:25:02,721 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 04:25:04,671 - INFO - Applying MX quantization groups to microsoft/phi-1_5
2025-10-15 04:25:04,673 - INFO - Replaced 48 Linear layers with MxLinear
2025-10-15 04:25:04,676 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-15 05:11:37,727 - INFO - phi-1_5-TierD                   MX-TierD         PPL 2.49  Xent 0.9119  Entr 2.7322
2025-10-15 05:12:02,454 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 05:12:05,902 - INFO - Replaced 144 Linear layers with NvFp8Linear
2025-10-15 05:12:29,227 - WARNING - Skipping NV-FP8 benchmark for phi-1_5 [D]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-15 05:12:29,228 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-15 05:12:29,228 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 05:36:20,335 - INFO - phi-1_5-INT8                    INT8             PPL 2.35  Xent 0.8544  Entr nan
2025-10-15 05:36:40,904 - INFO - 
================================================================================
2025-10-15 05:36:40,904 - INFO - Evaluating: llama-3.1-8b
2025-10-15 05:36:40,904 - INFO - ================================================================================
2025-10-15 05:36:40,905 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-15 05:36:40,916 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 06:18:36,509 - INFO - llama-3.1-8b                    Baseline         PPL 1.76  Xent 0.5661  Entr 1.5749
2025-10-15 06:19:08,687 - INFO - 
--- BF16 (per-tensor cast) ---
2025-10-15 06:19:08,689 - INFO - BF16             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 07:00:54,566 - INFO - llama-3.1-8b-BF16               BF16             PPL 1.76  Xent 0.5661  Entr 1.5749
2025-10-15 07:01:26,782 - INFO - 
--- Tier A: MLP-only MXFP8 (very safe) ---
2025-10-15 07:01:26,782 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-15 07:01:26,783 - INFO - BFP16-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 07:01:28,394 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 07:01:28,396 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-15 07:43:10,807 - INFO - llama-3.1-8b-BFP16-TierA        BFP16-TierA      PPL 1.76  Xent 0.5661  Entr 1.5749
2025-10-15 07:43:43,258 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-15 07:43:43,259 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-10-15 07:43:43,260 - INFO - NVFP8Emu-TierA   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 07:43:44,935 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 07:43:44,938 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-15 07:43:46,734 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 07:43:46,737 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-15 11:39:38,287 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierANVFP8Emu-TierA   PPL 1.76  Xent 0.5678  Entr 1.6190
2025-10-15 11:40:46,825 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierANVFP8Emu-TierA   PPL 1.85  Xent 0.6135  Entr 1.7855
2025-10-15 11:41:19,297 - INFO - MX-TierA         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 11:41:21,152 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-15 11:41:21,155 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-15 12:38:04,683 - INFO - llama-3.1-8b-TierA              MX-TierA         PPL 1.78  Xent 0.5789  Entr 1.6479
2025-10-15 12:38:37,499 - INFO - NVFP8-TierA      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 12:38:43,140 - INFO - Replaced 96 Linear layers with NvFp8Linear
2025-10-15 12:39:06,118 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [A]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-15 12:39:06,119 - INFO - 
--- Tier B: MLP + attention projections in MXFP8 (safe) ---
2025-10-15 12:39:06,123 - INFO - BFP16-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 12:39:07,770 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 12:39:07,775 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-15 13:20:39,576 - INFO - llama-3.1-8b-BFP16-TierB        BFP16-TierB      PPL 1.76  Xent 0.5661  Entr 1.5749
2025-10-15 13:21:12,057 - INFO - NVFP8Emu-TierB   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 13:21:13,713 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 13:21:13,718 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-15 13:21:15,591 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 13:21:15,596 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-15 18:24:52,372 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierBNVFP8Emu-TierB   PPL 1.72  Xent 0.5415  Entr 1.4811
2025-10-15 18:26:17,433 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierBNVFP8Emu-TierB   PPL 1.76  Xent 0.5656  Entr 1.5844
2025-10-15 18:26:49,739 - INFO - MX-TierB         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 18:26:51,545 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-15 18:26:51,551 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-15 19:27:48,829 - INFO - llama-3.1-8b-TierB              MX-TierB         PPL 1.75  Xent 0.5571  Entr 1.5486
2025-10-15 19:28:21,307 - INFO - NVFP8-TierB      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 19:28:27,446 - INFO - Replaced 224 Linear layers with NvFp8Linear
2025-10-15 19:28:50,297 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [B]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-15 19:28:50,297 - INFO - 
--- Tier C: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-10-15 19:28:50,298 - INFO - BFP16-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 19:28:51,982 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 19:28:51,987 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-15 20:10:14,277 - INFO - llama-3.1-8b-BFP16-TierC        BFP16-TierC      PPL 1.76  Xent 0.5661  Entr 1.5749
2025-10-15 20:10:46,442 - INFO - NVFP8Emu-TierC   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-15 20:10:48,292 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 20:10:48,298 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-15 20:10:50,249 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-15 20:10:50,254 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-16 01:13:49,819 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierCNVFP8Emu-TierC   PPL 1.72  Xent 0.5415  Entr 1.4811
2025-10-16 01:15:02,489 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierCNVFP8Emu-TierC   PPL 1.76  Xent 0.5656  Entr 1.5844
2025-10-16 01:15:34,869 - INFO - MX-TierC         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 01:15:36,529 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-16 01:15:36,532 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-16 01:15:36,535 - INFO - Replaced 128 Linear layers with MxLinear
2025-10-16 02:16:16,959 - INFO - llama-3.1-8b-TierC              MX-TierC         PPL 1.79  Xent 0.5822  Entr 1.6596
2025-10-16 02:16:49,428 - INFO - NVFP8-TierC      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 02:16:55,335 - INFO - Replaced 224 Linear layers with NvFp8Linear
2025-10-16 02:17:20,036 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [C]: Device compute capability 8.9 or higher required for FP8 execution.
2025-10-16 02:17:20,036 - INFO - 
--- Tier D: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-10-16 02:17:20,038 - INFO - BFP16-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 02:17:21,620 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-16 02:17:21,625 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-16 02:58:53,056 - INFO - llama-3.1-8b-BFP16-TierD        BFP16-TierD      PPL 1.76  Xent 0.5661  Entr 1.5749
2025-10-16 02:59:26,753 - INFO - NVFP8Emu-TierD   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 02:59:28,418 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-16 02:59:28,425 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-16 02:59:30,265 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-16 02:59:30,271 - INFO - Replaced 224 Linear layers with MxLinear
2025-10-16 08:02:30,544 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e4m3_no_shared-TierDNVFP8Emu-TierD   PPL 1.72  Xent 0.5415  Entr 1.4811
2025-10-16 08:03:45,785 - INFO - llama-3.1-8b-NVFP8-Emu-fp8_e5m2_no_shared-TierDNVFP8Emu-TierD   PPL 1.76  Xent 0.5656  Entr 1.5844
2025-10-16 08:04:18,585 - INFO - MX-TierD         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 08:04:20,297 - INFO - Applying MX quantization groups to meta-llama/Meta-Llama-3.1-8B
2025-10-16 08:04:20,300 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-16 08:04:20,303 - INFO - Replaced 128 Linear layers with MxLinear
2025-10-16 09:04:25,529 - INFO - llama-3.1-8b-TierD              MX-TierD         PPL 2.28  Xent 0.8225  Entr 2.4092
2025-10-16 09:05:00,342 - INFO - NVFP8-TierD      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 09:05:02,985 - WARNING - Skipping NV-FP8 benchmark for llama-3.1-8b [D]: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 12.38 MiB is free. Including non-PyTorch memory, this process has 2.37 GiB memory in use. Process 2245789 has 1.96 GiB memory in use. Process 2245783 has 2.51 GiB memory in use. Process 2245807 has 1.96 GiB memory in use. Process 2245799 has 4.47 GiB memory in use. Process 2245867 has 4.40 GiB memory in use. Process 2245896 has 1.96 GiB memory in use. Process 2245993 has 1.96 GiB memory in use. Process 2246032 has 1.96 GiB memory in use. Of the allocated memory 2.10 GiB is allocated by PyTorch, and 25.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-16 09:05:02,985 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-16 09:05:02,986 - INFO - INT8             Using GPUs: [1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-16 10:02:59,620 - INFO - llama-3.1-8b-INT8               INT8             PPL 1.82  Xent 0.5980  Entr nan
2025-10-16 10:03:20,151 - INFO - 
================================================================================
2025-10-16 10:03:20,152 - INFO - BENCHMARK COMPLETE
2025-10-16 10:03:20,152 - INFO - ================================================================================
2025-11-05 00:49:52,688 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-11-05 00:49:52,689 - INFO - Tier B selected: MLP + attention projections in MXFP8 (safe)
2025-11-05 00:49:52,689 - INFO - Tier C selected: MLP in MXFP6, attention projections in MXFP8 (balanced)
2025-11-05 00:49:52,689 - INFO - Tier D selected: MLP in MXFP4, attention projections in MXFP6 (aggressive)
2025-11-05 00:49:52,689 - INFO - ================================================================================
2025-11-05 00:49:52,689 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-05 00:49:52,689 - INFO - ================================================================================
2025-11-05 00:49:52,689 - INFO - 
================================================================================
2025-11-05 00:49:52,689 - INFO - DATASET: pile
2025-11-05 00:49:52,689 - INFO - ================================================================================
2025-11-05 00:49:52,689 - INFO - Loading evaluation data...
2025-11-05 00:49:52,689 - INFO - Loading 16 samples from The Pile (validation split)...
2025-11-05 00:49:56,139 - WARNING - Could not load The Pile: Bad split: validation. Available splits: ['train']. Falling back to WikiText.
2025-11-05 00:49:59,202 - INFO - Loaded 16 samples for dataset 'pile'
2025-11-05 00:49:59,203 - INFO - 
================================================================================
2025-11-05 00:49:59,203 - INFO - Evaluating: tinystories-33M
2025-11-05 00:49:59,203 - INFO - ================================================================================
2025-11-05 00:49:59,203 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 00:50:02,181 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:05,865 - INFO - tinystories-33M                 Baseline-pile    PPL 2.60  Xent 0.9542  Entr 2.6270
2025-11-05 00:50:06,320 - INFO - 
--- BF16 (per-tensor cast) ---
2025-11-05 00:50:06,344 - INFO - BF16-pile        Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:08,115 - INFO - tinystories-33M-BF16            BF16-pile        PPL 2.61  Xent 0.9609  Entr 2.6650
2025-11-05 00:50:08,442 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 00:50:08,442 - INFO - BFP16 MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "bfloat16",
    "a_elem_format": "bfloat16",
    "w_elem_format_bp": "bfloat16",
    "a_elem_format_bp": "bfloat16",
    "a_elem_format_bp_ex": "bfloat16",
    "a_elem_format_bp_os": "bfloat16",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 16,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-11-05 00:50:08,443 - INFO - BFP16-TA-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:09,816 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:09,817 - INFO - Replaced 8 Linear layers with MxLinear
2025-11-05 00:50:10,094 - INFO - tinystories-33M-BFP16-TA        BFP16-TA-pile    PPL 2.60  Xent 0.9543  Entr 2.6279
2025-11-05 00:50:10,513 - INFO - NV-FP8 Emulation (fp8_e4m3_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-11-05 00:50:10,513 - INFO - NV-FP8 Emulation (fp8_e5m2_no_shared) MX Configuration: {
    "scale_bits": 8,
    "w_elem_format": "fp8_e5m2",
    "a_elem_format": "fp8_e5m2",
    "w_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp": "fp8_e5m2",
    "a_elem_format_bp_ex": "fp8_e5m2",
    "a_elem_format_bp_os": "fp8_e5m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "none",
    "block_size": 0,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": false
}
2025-11-05 00:50:10,514 - INFO - NVFP8Emu-TA-pile  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:12,032 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:12,033 - INFO - Replaced 8 Linear layers with MxLinear
2025-11-05 00:50:12,647 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TANVFP8Emu-TA-pile  PPL 2.68  Xent 0.9850  Entr 2.7122
2025-11-05 00:50:13,686 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:13,687 - INFO - Replaced 8 Linear layers with MxLinear
2025-11-05 00:50:14,343 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TANVFP8Emu-TA-pile  PPL 2.82  Xent 1.0358  Entr 2.7304
2025-11-05 00:50:14,765 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 00:50:14,766 - INFO - MX-TA-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:16,165 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-11-05 00:50:16,166 - INFO - Replaced 8 Linear layers with MxLinear
2025-11-05 00:50:17,258 - INFO - tinystories-33M-TierA           MX-TA-pile       PPL 2.61  Xent 0.9596  Entr 2.6611
2025-11-05 00:50:17,682 - INFO - NVFP8-TA-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:19,292 - INFO - Replaced 8 Linear layers with NvFp8Linear
2025-11-05 00:50:19,696 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 00:50:19,696 - INFO - 
--- Tier TB: MLP + attention projections in MXFP8 (safe) ---
2025-11-05 00:50:19,697 - INFO - BFP16-TB-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:21,054 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:21,055 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:21,348 - INFO - tinystories-33M-BFP16-TB        BFP16-TB-pile    PPL 2.60  Xent 0.9548  Entr 2.6301
2025-11-05 00:50:21,807 - INFO - NVFP8Emu-TB-pile  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:23,257 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:23,258 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:24,368 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TBNVFP8Emu-TB-pile  PPL 2.56  Xent 0.9390  Entr 2.6149
2025-11-05 00:50:24,940 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:24,941 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:25,969 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TBNVFP8Emu-TB-pile  PPL 2.30  Xent 0.8307  Entr 2.3596
2025-11-05 00:50:26,387 - INFO - MX-TB-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:27,821 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-11-05 00:50:27,822 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:28,161 - INFO - tinystories-33M-TierB           MX-TB-pile       PPL 2.63  Xent 0.9675  Entr 2.6978
2025-11-05 00:50:28,577 - INFO - NVFP8-TB-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:30,208 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-11-05 00:50:30,615 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TB]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 00:50:30,616 - INFO - 
--- Tier TC: MLP in MXFP6, attention projections in MXFP8 (balanced) ---
2025-11-05 00:50:30,617 - INFO - BFP16-TC-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:32,032 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:32,032 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:32,320 - INFO - tinystories-33M-BFP16-TC        BFP16-TC-pile    PPL 2.60  Xent 0.9548  Entr 2.6301
2025-11-05 00:50:32,781 - INFO - NVFP8Emu-TC-pile  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:34,330 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:34,331 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:35,476 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TCNVFP8Emu-TC-pile  PPL 2.56  Xent 0.9390  Entr 2.6149
2025-11-05 00:50:35,753 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:35,754 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:37,077 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TCNVFP8Emu-TC-pile  PPL 2.30  Xent 0.8307  Entr 2.3596
2025-11-05 00:50:37,497 - INFO - MX Configuration (fp6_e3m2): {
    "scale_bits": 8,
    "w_elem_format": "fp6_e3m2",
    "a_elem_format": "fp6_e3m2",
    "w_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp": "fp6_e3m2",
    "a_elem_format_bp_ex": "fp6_e3m2",
    "a_elem_format_bp_os": "fp6_e3m2",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 00:50:37,498 - INFO - MX-TC-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:38,890 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-11-05 00:50:38,890 - INFO - Replaced 8 Linear layers with MxLinear
2025-11-05 00:50:38,891 - INFO - Replaced 16 Linear layers with MxLinear
2025-11-05 00:50:39,224 - INFO - tinystories-33M-TierC           MX-TC-pile       PPL 2.59  Xent 0.9508  Entr 2.6367
2025-11-05 00:50:39,642 - INFO - NVFP8-TC-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:41,109 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-11-05 00:50:41,514 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TC]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 00:50:41,514 - INFO - 
--- Tier TD: MLP in MXFP4, attention projections in MXFP6 (aggressive) ---
2025-11-05 00:50:41,515 - INFO - BFP16-TD-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:42,923 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:42,924 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:43,232 - INFO - tinystories-33M-BFP16-TD        BFP16-TD-pile    PPL 2.60  Xent 0.9548  Entr 2.6301
2025-11-05 00:50:43,691 - INFO - NVFP8Emu-TD-pile  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:45,085 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:45,086 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:46,171 - INFO - tinystories-33M-NVFP8-Emu-fp8_e4m3_no_shared-TDNVFP8Emu-TD-pile  PPL 2.56  Xent 0.9390  Entr 2.6149
2025-11-05 00:50:46,938 - INFO - Applying MX quantization to roneneldan/TinyStories-33M
2025-11-05 00:50:46,939 - INFO - Replaced 24 Linear layers with MxLinear
2025-11-05 00:50:48,011 - INFO - tinystories-33M-NVFP8-Emu-fp8_e5m2_no_shared-TDNVFP8Emu-TD-pile  PPL 2.30  Xent 0.8307  Entr 2.3596
2025-11-05 00:50:48,440 - INFO - MX Configuration (fp4_e2m1): {
    "scale_bits": 8,
    "w_elem_format": "fp4_e2m1",
    "a_elem_format": "fp4_e2m1",
    "w_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp": "fp4_e2m1",
    "a_elem_format_bp_ex": "fp4_e2m1",
    "a_elem_format_bp_os": "fp4_e2m1",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 00:50:48,440 - INFO - MX-TD-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:49,819 - INFO - Applying MX quantization groups to roneneldan/TinyStories-33M
2025-11-05 00:50:49,820 - INFO - Replaced 8 Linear layers with MxLinear
2025-11-05 00:50:49,820 - INFO - Replaced 16 Linear layers with MxLinear
2025-11-05 00:50:50,128 - INFO - tinystories-33M-TierD           MX-TD-pile       PPL 2.80  Xent 1.0286  Entr 2.7814
2025-11-05 00:50:50,560 - INFO - NVFP8-TD-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:52,332 - INFO - Replaced 24 Linear layers with NvFp8Linear
2025-11-05 00:50:52,757 - WARNING - Skipping NV-FP8 benchmark for tinystories-33M [TD]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 00:50:52,757 - INFO - 
--- INT8 (bitsandbytes) ---
2025-11-05 00:50:52,758 - INFO - INT8-pile        Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:50:56,190 - INFO - tinystories-33M-INT8            INT8-pile        PPL 2.65  Xent 0.9761  Entr nan
2025-11-05 00:50:56,486 - INFO - 
================================================================================
2025-11-05 00:50:56,486 - INFO - BENCHMARK COMPLETE
2025-11-05 00:50:56,487 - INFO - ================================================================================
2025-11-05 00:58:26,324 - INFO - Will replace ALL Linear layers with MX (overrides tier selection)
2025-11-05 00:58:26,325 - INFO - ================================================================================
2025-11-05 00:58:26,325 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-05 00:58:26,325 - INFO - ================================================================================
2025-11-05 00:58:26,325 - INFO - 
================================================================================
2025-11-05 00:58:26,325 - INFO - DATASET: pile
2025-11-05 00:58:26,325 - INFO - ================================================================================
2025-11-05 00:58:26,325 - INFO - Loading evaluation data...
2025-11-05 00:58:26,325 - INFO - Loading 512 samples from The Pile (validation split)...
2025-11-05 00:58:28,911 - WARNING - Could not load The Pile: Bad split: validation. Available splits: ['train']. Falling back to WikiText.
2025-11-05 00:58:31,074 - INFO - Loaded 512 samples for dataset 'pile'
2025-11-05 00:58:31,074 - INFO - 
================================================================================
2025-11-05 00:58:31,074 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 00:58:31,074 - INFO - ================================================================================
2025-11-05 00:58:31,075 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 00:58:33,650 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:59:10,040 - INFO - qwen1.5-0.5B                    Baseline-pile    PPL 8.97  Xent 2.1936  Entr 5.4012
2025-11-05 00:59:11,845 - INFO - 
--- Tier ALL: All Linear layers ---
2025-11-05 00:59:11,845 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 00:59:11,846 - INFO - Enabling output quantization for format fp8_e4m3
2025-11-05 00:59:11,901 - INFO - MX-ALL-pile      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 00:59:13,835 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 00:59:13,840 - INFO - Replaced 169 Linear layers with MxLinear
2025-11-05 01:00:04,037 - INFO - qwen1.5-0.5B-AllLinears-fp8_e4m3MX-ALL-pile      PPL 55.10  Xent 4.0092  Entr 7.0873
2025-11-05 01:00:05,764 - INFO - NVFP8-ALL-pile   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 01:00:08,213 - INFO - Replaced 169 Linear layers with NvFp8Linear
2025-11-05 01:00:10,349 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [ALL]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 01:00:10,350 - INFO - 
--- INT8 (bitsandbytes) ---
2025-11-05 01:00:10,352 - INFO - INT8-pile        Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 01:01:45,433 - INFO - qwen1.5-0.5B-INT8               INT8-pile        PPL 4.78  Xent 1.5634  Entr nan
2025-11-05 01:01:45,875 - INFO - 
================================================================================
2025-11-05 01:01:45,875 - INFO - DATASET: wikitext
2025-11-05 01:01:45,875 - INFO - ================================================================================
2025-11-05 01:01:45,875 - INFO - Loading evaluation data...
2025-11-05 01:01:47,444 - INFO - Loaded 512 samples for dataset 'wikitext'
2025-11-05 01:01:47,444 - INFO - 
================================================================================
2025-11-05 01:01:47,444 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 01:01:47,444 - INFO - ================================================================================
2025-11-05 01:01:47,444 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 01:01:47,445 - INFO - Baseline-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 01:02:19,592 - INFO - qwen1.5-0.5B                    Baseline-wikitext  PPL 7.80  Xent 2.0540  Entr 4.8007
2025-11-05 01:02:20,951 - INFO - 
--- Tier ALL: All Linear layers ---
2025-11-05 01:02:20,952 - INFO - Enabling output quantization for format fp8_e4m3
2025-11-05 01:02:20,953 - INFO - MX-ALL-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 01:02:22,831 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 01:02:22,835 - INFO - Replaced 169 Linear layers with MxLinear
2025-11-05 01:03:12,112 - INFO - qwen1.5-0.5B-AllLinears-fp8_e4m3MX-ALL-wikitext  PPL 8.09  Xent 2.0910  Entr 4.6638
2025-11-05 01:03:13,473 - INFO - NVFP8-ALL-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 01:03:15,786 - INFO - Replaced 169 Linear layers with NvFp8Linear
2025-11-05 01:03:17,437 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [ALL]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 01:03:17,437 - INFO - 
--- INT8 (bitsandbytes) ---
2025-11-05 01:03:17,438 - INFO - INT8-wikitext    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 01:04:51,341 - INFO - qwen1.5-0.5B-INT8               INT8-wikitext    PPL 7.79  Xent 2.0524  Entr nan
2025-11-05 01:04:51,741 - INFO - 
================================================================================
2025-11-05 01:04:51,741 - INFO - BENCHMARK COMPLETE
2025-11-05 01:04:51,742 - INFO - ================================================================================
2025-11-05 12:25:29,415 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-11-05 12:25:29,418 - INFO - ================================================================================
2025-11-05 12:25:29,418 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-05 12:25:29,418 - INFO - ================================================================================
2025-11-05 12:25:29,418 - INFO - 
================================================================================
2025-11-05 12:25:29,418 - INFO - DATASET: pile
2025-11-05 12:25:29,418 - INFO - ================================================================================
2025-11-05 12:25:29,418 - INFO - Loading evaluation data...
2025-11-05 12:25:29,418 - INFO - Loading 512 samples from The Pile (validation split)...
2025-11-05 12:25:31,994 - WARNING - Could not load The Pile: Bad split: validation. Available splits: ['train']. Falling back to WikiText.
2025-11-05 12:25:37,063 - INFO - Loaded 512 samples for dataset 'pile'
2025-11-05 12:25:37,064 - INFO - 
================================================================================
2025-11-05 12:25:37,064 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:25:37,064 - INFO - ================================================================================
2025-11-05 12:25:37,064 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:25:39,759 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:26:12,678 - INFO - qwen1.5-0.5B                    Baseline-pile    PPL 8.97  Xent 2.1936  Entr 5.4012
2025-11-05 12:26:14,454 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 12:26:14,454 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 12:26:14,499 - INFO - MX-TA-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:26:16,483 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:26:16,485 - INFO - Replaced 72 Linear layers with MxLinear
2025-11-05 12:27:31,669 - INFO - qwen1.5-0.5B-TierA              MX-TA-pile       PPL 29.54  Xent 3.3857  Entr 6.8641
2025-11-05 12:27:33,406 - INFO - NVFP8-TA-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:27:35,877 - INFO - Replaced 72 Linear layers with NvFp8Linear
2025-11-05 12:27:37,748 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:27:37,749 - INFO - 
--- INT8 (bitsandbytes) ---
2025-11-05 12:27:37,749 - INFO - INT8-pile        Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:29:12,593 - INFO - qwen1.5-0.5B-INT8               INT8-pile        PPL 4.78  Xent 1.5634  Entr nan
2025-11-05 12:29:12,956 - INFO - 
================================================================================
2025-11-05 12:29:12,957 - INFO - DATASET: wikitext
2025-11-05 12:29:12,957 - INFO - ================================================================================
2025-11-05 12:29:12,957 - INFO - Loading evaluation data...
2025-11-05 12:29:14,421 - INFO - Loaded 512 samples for dataset 'wikitext'
2025-11-05 12:29:14,421 - INFO - 
================================================================================
2025-11-05 12:29:14,421 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:29:14,421 - INFO - ================================================================================
2025-11-05 12:29:14,421 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:29:14,422 - INFO - Baseline-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:29:46,840 - INFO - qwen1.5-0.5B                    Baseline-wikitext  PPL 7.80  Xent 2.0540  Entr 4.8007
2025-11-05 12:29:48,515 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 12:29:48,520 - INFO - MX-TA-wikitext   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:29:50,368 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:29:50,371 - INFO - Replaced 72 Linear layers with MxLinear
2025-11-05 12:30:25,534 - INFO - qwen1.5-0.5B-TierA              MX-TA-wikitext   PPL 7.17  Xent 1.9700  Entr 4.5913
2025-11-05 12:30:27,232 - INFO - NVFP8-TA-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:30:29,796 - INFO - Replaced 72 Linear layers with NvFp8Linear
2025-11-05 12:30:31,083 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:30:31,084 - INFO - 
--- INT8 (bitsandbytes) ---
2025-11-05 12:30:31,085 - INFO - INT8-wikitext    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:32:07,840 - INFO - qwen1.5-0.5B-INT8               INT8-wikitext    PPL 7.79  Xent 2.0524  Entr nan
2025-11-05 12:32:08,326 - INFO - 
================================================================================
2025-11-05 12:32:08,326 - INFO - BENCHMARK COMPLETE
2025-11-05 12:32:08,326 - INFO - ================================================================================
2025-11-05 12:42:55,193 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-11-05 12:42:55,193 - INFO - ================================================================================
2025-11-05 12:42:55,193 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-05 12:42:55,193 - INFO - ================================================================================
2025-11-05 12:42:55,193 - INFO - 
================================================================================
2025-11-05 12:42:55,193 - INFO - DATASET: pile
2025-11-05 12:42:55,193 - INFO - ================================================================================
2025-11-05 12:42:55,194 - INFO - Loading evaluation data...
2025-11-05 12:42:55,194 - INFO - Loading 512 samples from The Pile (train split)...
2025-11-05 12:42:58,655 - INFO - Loaded 512 samples from The Pile
2025-11-05 12:42:58,655 - INFO - Loaded 512 samples for dataset 'pile'
2025-11-05 12:42:58,656 - INFO - 
================================================================================
2025-11-05 12:42:58,656 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:42:58,656 - INFO - ================================================================================
2025-11-05 12:42:58,656 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:43:01,249 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:43:34,834 - INFO - qwen1.5-0.5B                    Baseline-pile    PPL 3.33  Xent 1.2018  Entr 3.1732
2025-11-05 12:43:36,795 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 12:43:36,796 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 12:43:36,831 - INFO - MX-TA-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:43:38,773 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:43:38,775 - INFO - Replaced 72 Linear layers with MxLinear
2025-11-05 12:44:50,182 - INFO - qwen1.5-0.5B-TierA              MX-TA-pile       PPL 3.24  Xent 1.1766  Entr 3.0907
2025-11-05 12:44:51,883 - INFO - NVFP8-TA-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:44:54,462 - INFO - Replaced 72 Linear layers with NvFp8Linear
2025-11-05 12:44:56,576 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:44:56,577 - INFO - 
--- INT8 (bitsandbytes) --- Skipped (dependency or CUDA unavailable)
2025-11-05 12:44:56,577 - INFO - 
================================================================================
2025-11-05 12:44:56,577 - INFO - DATASET: wikitext
2025-11-05 12:44:56,577 - INFO - ================================================================================
2025-11-05 12:44:56,577 - INFO - Loading evaluation data...
2025-11-05 12:44:59,346 - INFO - Loaded 512 samples for dataset 'wikitext'
2025-11-05 12:44:59,346 - INFO - 
================================================================================
2025-11-05 12:44:59,346 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:44:59,346 - INFO - ================================================================================
2025-11-05 12:44:59,346 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:44:59,347 - INFO - Baseline-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:45:32,334 - INFO - qwen1.5-0.5B                    Baseline-wikitext  PPL 7.80  Xent 2.0540  Entr 4.8007
2025-11-05 12:45:34,077 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 12:45:34,078 - INFO - MX-TA-wikitext   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:45:35,984 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:45:35,987 - INFO - Replaced 72 Linear layers with MxLinear
2025-11-05 12:46:10,765 - INFO - qwen1.5-0.5B-TierA              MX-TA-wikitext   PPL 7.17  Xent 1.9700  Entr 4.5913
2025-11-05 12:46:12,094 - INFO - NVFP8-TA-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:46:14,842 - INFO - Replaced 72 Linear layers with NvFp8Linear
2025-11-05 12:46:16,061 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:46:16,061 - INFO - 
--- INT8 (bitsandbytes) --- Skipped (dependency or CUDA unavailable)
2025-11-05 12:46:16,063 - INFO - 
================================================================================
2025-11-05 12:46:16,063 - INFO - BENCHMARK COMPLETE
2025-11-05 12:46:16,063 - INFO - ================================================================================
2025-11-05 12:47:38,141 - INFO - Tier A selected: MLP-only MXFP8 (very safe)
2025-11-05 12:47:38,141 - INFO - ================================================================================
2025-11-05 12:47:38,142 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-05 12:47:38,142 - INFO - ================================================================================
2025-11-05 12:47:38,142 - INFO - 
================================================================================
2025-11-05 12:47:38,142 - INFO - DATASET: pile
2025-11-05 12:47:38,142 - INFO - ================================================================================
2025-11-05 12:47:38,142 - INFO - Loading evaluation data...
2025-11-05 12:47:38,142 - INFO - Loading 512 samples from The Pile (train split)...
2025-11-05 12:47:41,167 - INFO - Loaded 512 samples from The Pile
2025-11-05 12:47:41,167 - INFO - Loaded 512 samples for dataset 'pile'
2025-11-05 12:47:41,167 - INFO - 
================================================================================
2025-11-05 12:47:41,167 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:47:41,167 - INFO - ================================================================================
2025-11-05 12:47:41,167 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:47:43,633 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:48:17,338 - INFO - qwen1.5-0.5B                    Baseline-pile    PPL 3.33  Xent 1.2018  Entr 3.1732
2025-11-05 12:48:19,348 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 12:48:19,349 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 12:48:19,374 - INFO - MX-TA-pile       Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:48:21,285 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:48:21,288 - INFO - Replaced 72 Linear layers with MxLinear
2025-11-05 12:48:56,839 - INFO - qwen1.5-0.5B-TierA              MX-TA-pile       PPL 3.24  Xent 1.1766  Entr 3.0907
2025-11-05 12:48:58,770 - INFO - NVFP8-TA-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:49:01,277 - INFO - Replaced 72 Linear layers with NvFp8Linear
2025-11-05 12:49:03,767 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:49:03,768 - INFO - 
--- INT8 (bitsandbytes) --- Skipped (dependency or CUDA unavailable)
2025-11-05 12:49:03,768 - INFO - 
================================================================================
2025-11-05 12:49:03,768 - INFO - DATASET: wikitext
2025-11-05 12:49:03,768 - INFO - ================================================================================
2025-11-05 12:49:03,768 - INFO - Loading evaluation data...
2025-11-05 12:49:06,360 - INFO - Loaded 512 samples for dataset 'wikitext'
2025-11-05 12:49:06,360 - INFO - 
================================================================================
2025-11-05 12:49:06,360 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:49:06,360 - INFO - ================================================================================
2025-11-05 12:49:06,360 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:49:06,363 - INFO - Baseline-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:49:39,208 - INFO - qwen1.5-0.5B                    Baseline-wikitext  PPL 7.80  Xent 2.0540  Entr 4.8007
2025-11-05 12:49:41,085 - INFO - 
--- Tier TA: MLP-only MXFP8 (very safe) ---
2025-11-05 12:49:41,086 - INFO - MX-TA-wikitext   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:49:43,012 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:49:43,014 - INFO - Replaced 72 Linear layers with MxLinear
2025-11-05 12:50:17,970 - INFO - qwen1.5-0.5B-TierA              MX-TA-wikitext   PPL 7.17  Xent 1.9700  Entr 4.5913
2025-11-05 12:50:20,107 - INFO - NVFP8-TA-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:50:22,550 - INFO - Replaced 72 Linear layers with NvFp8Linear
2025-11-05 12:50:24,342 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [TA]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:50:24,343 - INFO - 
--- INT8 (bitsandbytes) --- Skipped (dependency or CUDA unavailable)
2025-11-05 12:50:24,343 - INFO - 
================================================================================
2025-11-05 12:50:24,343 - INFO - BENCHMARK COMPLETE
2025-11-05 12:50:24,343 - INFO - ================================================================================
2025-11-05 12:54:25,244 - INFO - Will replace ALL Linear layers with MX (overrides tier selection)
2025-11-05 12:54:25,244 - INFO - ================================================================================
2025-11-05 12:54:25,244 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-05 12:54:25,244 - INFO - ================================================================================
2025-11-05 12:54:25,244 - INFO - 
================================================================================
2025-11-05 12:54:25,244 - INFO - DATASET: pile
2025-11-05 12:54:25,244 - INFO - ================================================================================
2025-11-05 12:54:25,244 - INFO - Loading evaluation data...
2025-11-05 12:54:25,244 - INFO - Loading 512 samples from The Pile (train split)...
2025-11-05 12:54:35,418 - INFO - Loaded 512 samples from The Pile
2025-11-05 12:54:35,418 - INFO - Loaded 512 samples for dataset 'pile'
2025-11-05 12:54:35,419 - INFO - 
================================================================================
2025-11-05 12:54:35,419 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:54:35,419 - INFO - ================================================================================
2025-11-05 12:54:35,419 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:54:37,885 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:55:13,765 - INFO - qwen1.5-0.5B                    Baseline-pile    PPL 3.33  Xent 1.2018  Entr 3.1732
2025-11-05 12:55:15,581 - INFO - 
--- Tier ALL: All Linear layers ---
2025-11-05 12:55:15,581 - INFO - MX Configuration (fp8_e4m3): {
    "scale_bits": 8,
    "w_elem_format": "fp8_e4m3",
    "a_elem_format": "fp8_e4m3",
    "w_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp": "fp8_e4m3",
    "a_elem_format_bp_ex": "fp8_e4m3",
    "a_elem_format_bp_os": "fp8_e4m3",
    "mx_flush_fp32_subnorms": false,
    "shared_exp_method": "max",
    "block_size": 32,
    "bfloat": 0,
    "fp": 0,
    "bfloat_subnorms": true,
    "quantize_backprop": false,
    "round": "even",
    "round_m": "even",
    "round_weight": "even",
    "round_output": "even",
    "round_grad_weight": "even",
    "round_grad_input": "even",
    "round_mx_output": "even",
    "round_mx_input_grad_input": "even",
    "round_mx_weight_grad_input": "even",
    "round_mx_grad_output_grad_input": "even",
    "round_mx_input_grad_weight": "even",
    "round_mx_grad_output_grad_weight": "even",
    "softmax_exp2": false,
    "vec_use_exp2": false,
    "vec_use_recip": false,
    "custom_cuda": true
}
2025-11-05 12:55:15,582 - INFO - Enabling output quantization for format fp8_e4m3
2025-11-05 12:55:15,607 - INFO - MX-ALL-pile      Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:55:17,499 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:55:17,504 - INFO - Replaced 169 Linear layers with MxLinear
2025-11-05 12:56:12,695 - INFO - qwen1.5-0.5B-AllLinears-fp8_e4m3MX-ALL-pile      PPL 3.89  Xent 1.3583  Entr 3.2332
2025-11-05 12:56:14,492 - INFO - NVFP8-ALL-pile   Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:56:16,804 - INFO - Replaced 169 Linear layers with NvFp8Linear
2025-11-05 12:56:18,783 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [ALL]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:56:18,783 - INFO - 
--- INT8 (bitsandbytes) --- Skipped (dependency or CUDA unavailable)
2025-11-05 12:56:18,783 - INFO - 
================================================================================
2025-11-05 12:56:18,783 - INFO - DATASET: wikitext
2025-11-05 12:56:18,783 - INFO - ================================================================================
2025-11-05 12:56:18,783 - INFO - Loading evaluation data...
2025-11-05 12:56:23,503 - INFO - Loaded 512 samples for dataset 'wikitext'
2025-11-05 12:56:23,503 - INFO - 
================================================================================
2025-11-05 12:56:23,503 - INFO - Evaluating: qwen1.5-0.5B
2025-11-05 12:56:23,503 - INFO - ================================================================================
2025-11-05 12:56:23,503 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-05 12:56:23,504 - INFO - Baseline-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:56:58,487 - INFO - qwen1.5-0.5B                    Baseline-wikitext  PPL 7.80  Xent 2.0540  Entr 4.8007
2025-11-05 12:57:00,448 - INFO - 
--- Tier ALL: All Linear layers ---
2025-11-05 12:57:00,448 - INFO - Enabling output quantization for format fp8_e4m3
2025-11-05 12:57:00,451 - INFO - MX-ALL-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:57:02,357 - INFO - Applying MX quantization groups to Qwen/Qwen1.5-0.5B
2025-11-05 12:57:02,361 - INFO - Replaced 169 Linear layers with MxLinear
2025-11-05 12:57:56,687 - INFO - qwen1.5-0.5B-AllLinears-fp8_e4m3MX-ALL-wikitext  PPL 8.09  Xent 2.0910  Entr 4.6638
2025-11-05 12:57:58,416 - INFO - NVFP8-ALL-wikitext  Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-11-05 12:58:00,949 - INFO - Replaced 169 Linear layers with NvFp8Linear
2025-11-05 12:58:03,256 - WARNING - Skipping NV-FP8 benchmark for qwen1.5-0.5B [ALL]: Device compute capability 8.9 or higher required for FP8 execution.
2025-11-05 12:58:03,256 - INFO - 
--- INT8 (bitsandbytes) --- Skipped (dependency or CUDA unavailable)
2025-11-05 12:58:03,256 - INFO - 
================================================================================
2025-11-05 12:58:03,256 - INFO - BENCHMARK COMPLETE
2025-11-05 12:58:03,256 - INFO - ================================================================================
2025-11-07 11:40:09,228 - INFO - Will replace ALL Linear layers with MX (overrides tier selection)
2025-11-07 11:40:09,229 - INFO - ================================================================================
2025-11-07 11:40:09,229 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-11-07 11:40:09,229 - INFO - ================================================================================
2025-11-07 11:40:09,229 - INFO - 
================================================================================
2025-11-07 11:40:09,229 - INFO - DATASET: pile
2025-11-07 11:40:09,229 - INFO - ================================================================================
2025-11-07 11:40:09,229 - INFO - Loading evaluation data...
2025-11-07 11:40:09,229 - INFO - Loading 512 samples from The Pile (train split)...
2025-11-07 11:40:13,878 - INFO - Loaded 512 samples from The Pile
2025-11-07 11:40:13,878 - INFO - Loaded 512 samples for dataset 'pile'
2025-11-07 11:40:13,878 - INFO - 
================================================================================
2025-11-07 11:40:13,878 - INFO - Evaluating: qwen1.5-0.5B
2025-11-07 11:40:13,878 - INFO - ================================================================================
2025-11-07 11:40:13,878 - INFO - 
--- Baseline (FP32/BF16) ---
2025-11-07 11:40:16,577 - INFO - Baseline-pile    Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
