nohup: ignoring input
/scratch/ashvin/mx-scaling-tests/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/scratch/ashvin/mx-scaling-tests/.venv/lib/python3.12/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
2025-10-03 13:38:57,396 - INFO - Tier A selected. Will replace modules matching: ['gate_proj', 'up_proj', 'down_proj', 'fc1', 'fc2', 'w1', 'w2', 'w3', 'dense_h_to_4h', 'dense_4h_to_h', 'mlp.c_fc', 'mlp.c_proj']
2025-10-03 13:38:58,594 - INFO - MX Configuration: {'w_elem_format': 'fp8_e4m3', 'a_elem_format': 'fp8_e4m3', 'scale_bits': 8, 'block_size': 32, 'custom_cuda': True, 'quantize_backprop': False, 'round': 'even', 'w_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp': 'fp8_e4m3', 'a_elem_format_bp_os': 'fp8_e4m3', 'a_elem_format_bp_ex': 'fp8_e4m3', 'round_m': 'even', 'round_output': 'even', 'round_grad_weight': 'even', 'round_grad_input': 'even', 'round_weight': 'even', 'round_mx_output': 'even', 'round_mx_input_grad_input': 'even', 'round_mx_weight_grad_input': 'even', 'round_mx_grad_output_grad_input': 'even', 'round_mx_input_grad_weight': 'even', 'round_mx_grad_output_grad_weight': 'even'}
2025-10-03 13:38:58,594 - INFO - Loading evaluation data...
2025-10-03 13:38:58,594 - INFO - Loading 200 samples from The Pile (train split)...
2025-10-03 13:39:01,505 - INFO - Loaded 200 samples from The Pile
2025-10-03 13:39:01,506 - INFO - ================================================================================
2025-10-03 13:39:01,506 - INFO - SELECTIVE MX GEMM REPLACEMENT BENCHMARK
2025-10-03 13:39:01,506 - INFO - ================================================================================
2025-10-03 13:39:01,506 - INFO - 
================================================================================
2025-10-03 13:39:01,506 - INFO - Evaluating: phi-1_5
2025-10-03 13:39:01,506 - INFO - ================================================================================
2025-10-03 13:39:01,506 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:39:03,277 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:40:00,350 - INFO - phi-1_5                         Baseline         PPL 2.38  Xent 0.8663  Entr 2.4644
2025-10-03 13:40:15,043 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:40:15,110 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:40:16,726 - INFO - Applying MX quantization to microsoft/phi-1_5
2025-10-03 13:40:16,728 - INFO - Replaced 48 Linear layers with MxLinear
/scratch/ashvin/mx-scaling-tests/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
2025-10-03 13:40:45,029 - INFO - phi-1_5-MX                      MX-Selective     PPL 2.42  Xent 0.8851  Entr 2.5469
2025-10-03 13:40:59,748 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:40:59,751 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:41:00,141 - WARNING - Skipping INT8 benchmark for phi-1_5: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:41:00,141 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:41:00,142 - INFO - 
================================================================================
2025-10-03 13:41:00,142 - INFO - Evaluating: llama-3.1-8b
2025-10-03 13:41:00,142 - INFO - ================================================================================
2025-10-03 13:41:00,142 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:41:00,143 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 59.62it/s]
2025-10-03 13:41:30,394 - INFO - llama-3.1-8b                    Baseline         PPL 1.76  Xent 0.5670  Entr 1.5463
2025-10-03 13:41:53,545 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:41:53,546 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 55.87it/s]
2025-10-03 13:41:55,090 - INFO - Applying MX quantization to meta-llama/Meta-Llama-3.1-8B
2025-10-03 13:41:55,092 - INFO - Replaced 96 Linear layers with MxLinear
2025-10-03 13:42:32,102 - INFO - llama-3.1-8b-MX                 MX-Selective     PPL 1.78  Xent 0.5755  Entr 1.6134
2025-10-03 13:42:55,250 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:42:55,252 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:42:55,644 - WARNING - Skipping INT8 benchmark for llama-3.1-8b: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:42:55,644 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:42:55,644 - INFO - 
================================================================================
2025-10-03 13:42:55,645 - INFO - Evaluating: qwen2.5-14b
2025-10-03 13:42:55,645 - INFO - ================================================================================
2025-10-03 13:42:55,645 - INFO - Using batch size override 1 for qwen2.5-14b
2025-10-03 13:42:55,645 - INFO - 
--- Baseline (FP32/BF16) ---
2025-10-03 13:42:55,646 - INFO - Baseline         Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:06,  1.09it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:05,  1.07it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:02<00:04,  1.05it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.04it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:04<00:02,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:05<00:01,  1.04it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:06<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:07<00:00,  1.13it/s]
2025-10-03 13:43:04,212 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 13:43:57,692 - INFO - qwen2.5-14b                     Baseline         PPL 2.35  Xent 0.8533  Entr 2.1449
2025-10-03 13:44:08,121 - INFO - 
--- MX (Selective GEMM) ---
2025-10-03 13:44:08,123 - INFO - MX-Selective     Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:06,  1.16it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:05,  1.14it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:02<00:04,  1.13it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.12it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:04<00:02,  1.12it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:05<00:01,  1.10it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:06<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.20it/s]
2025-10-03 13:44:17,438 - INFO - Loaded Qwen/Qwen2.5-14B across devices: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3', 'cuda:4', 'cuda:5', 'cuda:6', 'cuda:7']
2025-10-03 13:44:17,440 - INFO - Applying MX quantization to Qwen/Qwen2.5-14B
2025-10-03 13:44:17,444 - INFO - Replaced 144 Linear layers with MxLinear
2025-10-03 13:46:06,421 - INFO - qwen2.5-14b-MX                  MX-Selective     PPL 2.33  Xent 0.8438  Entr 2.1283
2025-10-03 13:46:16,833 - INFO - 
--- INT8 (bitsandbytes) ---
2025-10-03 13:46:16,835 - INFO - INT8             Using GPUs: [0, 1, 2, 3, 4, 5, 6, 7] with 1 worker(s)/device
2025-10-03 13:46:18,084 - WARNING - Skipping INT8 benchmark for qwen2.5-14b: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`
2025-10-03 13:46:18,084 - INFO - 
--- NV-FP8 (Transformer Engine) --- Skipped (dependency or CUDA unavailable)
2025-10-03 13:46:18,084 - INFO - 
================================================================================
2025-10-03 13:46:18,084 - INFO - BENCHMARK COMPLETE
2025-10-03 13:46:18,084 - INFO - ================================================================================
